{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-20T14:14:38.384634Z","iopub.status.busy":"2023-10-20T14:14:38.384351Z","iopub.status.idle":"2023-10-20T14:14:51.604707Z","shell.execute_reply":"2023-10-20T14:14:51.603765Z","shell.execute_reply.started":"2023-10-20T14:14:38.384610Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n"]}],"source":["!pip install faiss-gpu"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-20T14:14:51.607173Z","iopub.status.busy":"2023-10-20T14:14:51.606811Z","iopub.status.idle":"2023-10-20T14:15:02.917638Z","shell.execute_reply":"2023-10-20T14:15:02.916499Z","shell.execute_reply.started":"2023-10-20T14:14:51.607138Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting neptune-client\n","  Downloading neptune_client-1.8.2-py3-none-any.whl (465 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.2)\n","Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.12.3)\n","Collecting open_clip_torch\n","  Downloading open_clip_torch-2.22.0-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\n","Requirement already satisfied: albumentations in /opt/conda/lib/python3.10/site-packages (1.3.1)\n","Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (4.8.0.74)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.15.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.65.0)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (6.0)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.11.1)\n","Collecting loguru\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.0.4)\n","Collecting vit_pytorch\n","  Downloading vit_pytorch-1.6.1-py3-none-any.whl (98 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (3.1.31)\n","Requirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (9.5.0)\n","Requirement already satisfied: PyJWT in /opt/conda/lib/python3.10/site-packages (from neptune-client) (2.7.0)\n","Requirement already satisfied: boto3>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (1.26.100)\n","Collecting bravado<12.0.0,>=11.0.0 (from neptune-client)\n","  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (8.1.3)\n","Requirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (0.18.3)\n","Requirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (3.2.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from neptune-client) (21.3)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from neptune-client) (1.5.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from neptune-client) (5.9.3)\n","Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (2.31.0)\n","Requirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (1.3.1)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (1.16.0)\n","Collecting swagger-spec-validator>=2.7.4 (from neptune-client)\n","  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (1.26.15)\n","Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.10/site-packages (from neptune-client) (1.6.0)\n","Requirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.0.0)\n","Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.4)\n","Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.3.1)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.51.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.20.0)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.3)\n","Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.23.5)\n","Requirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (59.8.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.3.6)\n","Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.40.0)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (2023.6.3)\n","Collecting ftfy (from open_clip_torch)\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open_clip_torch) (0.1.99)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.21.0)\n","Requirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.8.0.74)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2023.6.0)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.0.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.6.3)\n","Requirement already satisfied: lightning-utilities>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.9.0)\n","Collecting einops>=0.7.0 (from vit_pytorch)\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.0->neptune-client) (1.29.161)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.0->neptune-client) (1.0.1)\n","Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.16.0->neptune-client) (0.6.1)\n","Collecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune-client)\n","  Downloading bravado_core-6.1.0-py2.py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.5)\n","Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n","Requirement already satisfied: simplejson in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.19.1)\n","Collecting monotonic (from bravado<12.0.0,>=11.0.0->neptune-client)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=2.0.8->neptune-client) (4.0.10)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.7)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->neptune-client) (3.0.9)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.10/site-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune-client) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune-client) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune-client) (2023.5.7)\n","Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n","Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2.31.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n","Requirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16.1->albumentations) (0.2)\n","Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.17.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n","Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->open_clip_torch) (0.2.6)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->neptune-client) (2023.3)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n","Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client)\n","  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.19.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n","Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n","Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n","Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.0)\n","Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n","Collecting rfc3987 (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client)\n","  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.0)\n","Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.13)\n","Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.3)\n","Installing collected packages: rfc3987, monotonic, loguru, jsonref, ftfy, einops, swagger-spec-validator, vit_pytorch, open_clip_torch, bravado-core, bravado, neptune-client\n","Successfully installed bravado-11.0.3 bravado-core-6.1.0 einops-0.7.0 ftfy-6.1.1 jsonref-1.1.0 loguru-0.7.2 monotonic-1.6 neptune-client-1.8.2 open_clip_torch-2.22.0 rfc3987-1.3.8 swagger-spec-validator-3.0.3 vit_pytorch-1.6.1\n"]}],"source":["!pip install neptune-client timm tensorboard open_clip_torch transformers albumentations opencv-python torchvision tqdm PyYAML scipy loguru transformers pytorch_lightning vit_pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-20T14:15:02.919477Z","iopub.status.busy":"2023-10-20T14:15:02.919099Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import glob\n","import sys\n","import os\n","import time\n","import random\n","import math\n","\n","# DATALOADER\n","import cv2\n","from PIL import Image\n","import numpy as np\n","import albumentations as A\n","import torchvision.transforms as T\n","from PIL import Image\n","import pandas as pd\n","\n","# BUILDING MODEL\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# TRAINING\n","from torch.utils.data import DataLoader, Dataset\n","import faiss\n","from tqdm import tqdm_notebook as tqdm\n","\n","# OTHER STUFF\n","import timm\n","from transformers import (get_linear_schedule_with_warmup, \n","                          get_constant_schedule,\n","                          get_cosine_schedule_with_warmup, \n","                          get_cosine_with_hard_restarts_schedule_with_warmup,\n","                          get_constant_schedule_with_warmup)\n","import gc\n","import transformers\n","from transformers import CLIPProcessor, CLIPVisionModel,  CLIPVisionConfig\n","# from pytorch_metric_learning import losses\n","import open_clip\n","\n","# UTILS\n","# import vpr_utils as utilities\n","\n","# %load_ext autoreload\n","# %autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["open_clip.list_pretrained()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    model_name = 'ViT-L-14-336'\n","    model_data = 'openai'\n","    samples_per_class = 50\n","    n_classes = 0\n","    min_samples = 4\n","    image_size = 336\n","    hidden_layer = 768\n","    seed = 5\n","    workers = 12\n","    train_batch_size = 16\n","    valid_batch_size = 32 \n","    emb_size = 768\n","    vit_bb_lr = {'10': 1.25e-6, '20': 2.5e-6, '26': 5e-6, '32': 10e-6} \n","    vit_bb_wd = 1e-3\n","    hd_lr = 3e-4\n","    hd_wd = 1e-5\n","    autocast = True\n","    n_warmup_steps = 1000\n","    n_epochs = 2\n","    device = torch.device('cuda')\n","    s=30.\n","    m=.45\n","    m_min=.05\n","    acc_steps = 4\n","    global_step = 0\n","    reduce_lr = 0.1\n","    crit = 'ce'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CFG.device"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["utilities.set_seed(CFG.seed)"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# used for training\n","training_samples = []\n","values_counts = []\n","num_classes = 0\n","\n","# # H&M\n","# files = glob.glob(\"../H&M/images/*/*\")\n","# file_paths = dict((os.path.splitext(os.path.split(f)[-1])[0], f) for f in files)\n","\n","# df = pd.read_csv('../H&M/articles.csv', \n","#                  usecols=['article_id', 'product_code'],\n","#                  dtype={'article_id': str, 'product_code': str})\n","\n","groupped_products = {}\n","# for index, row in df.iterrows():\n","#     v = groupped_products.get(row['product_code'], [])\n","#     f = file_paths.get(row['article_id'])\n","#     if f:\n","#         groupped_products[row['product_code']] = v + [f]\n","\n","\n","# for key, value in groupped_products.items():\n","#     if len(value) >= CFG.min_samples:\n","#         paths = value[:CFG.samples_per_class]\n","        \n","#         values_counts.append(len(paths))\n","#         training_samples.extend([\n","#             (p, num_classes) for p in paths\n","#         ])\n","#         num_classes += 1\n","\n","# Product-10k\n","df = pd.read_csv('/kaggle/input/products-10k/products-10k/train.csv')\n","df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n","\n","\n","train_df = pd.read_csv('/kaggle/input/products-10k/products-10k/train.csv')\n","train_df['path'] = train_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/train' + '/' + x['name'], axis=1)\n","\n","\n","# remove ../products-10k/test/9397815.jpg from the list!\n","test_df = pd.read_csv('/kaggle/input/vprtestcsv/test_kaggletest.csv')\n","test_df = test_df.drop(test_df[test_df.name == '9397815.jpg'].index) # smt wrong with this img\n","test_df['path'] = test_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/test' + '/' + x['name'], axis=1)\n","\n","df = pd.concat([\n","    test_df[['class','path']],\n","    train_df[['class', 'path']]\n","])\n","df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n","\n","\n","for group in tqdm(set(df_g['class'])):\n","    names = list(df_g.path[df_g['class'] == group])\n","    if len(names) >= CFG.min_samples:\n","        paths = [\n","            name for name in names[:CFG.samples_per_class]\n","        ]\n","\n","        values_counts.append(len(paths))\n","        training_samples.extend([\n","            (p, num_classes) for p in paths\n","        ])\n","        \n","        num_classes += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # used for training\n","# training_samples = []\n","# values_counts = []\n","# num_classes = 0\n","\n","# # # H&M\n","# # files = glob.glob(\"../H&M/images/*/*\")\n","# # file_paths = dict((os.path.splitext(os.path.split(f)[-1])[0], f) for f in files)\n","\n","# # df = pd.read_csv('../H&M/articles.csv', \n","# #                  usecols=['article_id', 'product_code'],\n","# #                  dtype={'article_id': str, 'product_code': str})\n","\n","# groupped_products = {}\n","# # for index, row in df.iterrows():\n","# #     v = groupped_products.get(row['product_code'], [])\n","# #     f = file_paths.get(row['article_id'])\n","# #     if f:\n","# #         groupped_products[row['product_code']] = v + [f]\n","\n","\n","# # for key, value in groupped_products.items():\n","# #     if len(value) >= CFG.min_samples:\n","# #         paths = value[:CFG.samples_per_class]\n","        \n","# #         values_counts.append(len(paths))\n","# #         training_samples.extend([\n","# #             (p, num_classes) for p in paths\n","# #         ])\n","# #         num_classes += 1\n","\n","# # Product-10k\n","# df = pd.read_csv('/kaggle/input/products-10k/products-10k/train.csv')\n","# df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n","\n","\n","# # remove ../products-10k/test/9397815.jpg from the list!\n","# test_df = pd.read_csv('/kaggle/input/vprtestcsv/test_kaggletest.csv')\n","# test_df = test_df.drop(test_df[test_df.name == '9397815.jpg'].index) # smt wrong with this img\n","# test_df['path'] = test_df.apply(lambda x: '/kaggle/input/products-10k/products-10k/test' + '/' + x['name'], axis=1)\n","\n","# df = pd.concat([\n","#     test_df[['class','path']]\n","# ])\n","# df_g = df.groupby('class', group_keys=True).apply(lambda x: x)\n","\n","\n","# for group in tqdm(set(df_g['class'])):\n","#     names = list(df_g.path[df_g['class'] == group])\n","#     if len(names) >= CFG.min_samples:\n","#         paths = [\n","#             name for name in names[:CFG.samples_per_class]\n","#         ]\n","\n","#         values_counts.append(len(paths))\n","#         training_samples.extend([\n","#             (p, num_classes) for p in paths\n","#         ])\n","        \n","#         num_classes += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(data_train), CFG.n_classes "]},{"cell_type":"markdown","metadata":{},"source":["## CLIP Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Head(nn.Module):\n","    def __init__(self, hidden_size, k=3):\n","        super(Head, self).__init__()\n","        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n","        self.dropout = utilities.Multisample_Dropout()\n","        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n","        \n","    def forward(self, x):\n","        embeddings = self.dropout(x, self.emb)\n","        output = self.arc(embeddings)\n","        return output, F.normalize(embeddings)\n","    \n","class HeadV2(nn.Module):\n","    def __init__(self, hidden_size, k=3):\n","        super(HeadV2, self).__init__()\n","        self.arc = utilities.ArcMarginProduct_subcenter(hidden_size, CFG.n_classes, k)\n","        \n","    def forward(self, x):\n","        output = self.arc(x)\n","        return output, F.normalize(x)\n","    \n","class HeadV3(nn.Module):\n","    def __init__(self, hidden_size, k=3):\n","        super(HeadV3, self).__init__()        \n","        self.emb = nn.Linear(hidden_size, CFG.emb_size, bias=False)\n","        self.dropout = nn.Dropout1d(0.2)\n","        self.arc = utilities.ArcMarginProduct_subcenter(CFG.emb_size, CFG.n_classes, k)\n","        \n","    def forward(self, x):\n","        x = self.dropout(x)\n","        x = self.emb(x)\n","        output = self.arc(x)\n","        return output, F.normalize(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, vit_backbone, head_size, version='v1', k=3):\n","        super(Model, self).__init__()\n","        if version == 'v1':\n","            self.head = Head(head_size, k)\n","        elif version == 'v2':\n","            self.head = HeadV2(head_size, k)\n","        elif version == 'v3':\n","            self.head = HeadV3(head_size, k)\n","        else:\n","            self.head = Head(head_size, k)\n","        \n","        self.encoder = vit_backbone.visual\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        return self.head(x)\n","\n","    def get_parameters(self):\n","\n","        parameter_settings = [] \n","        parameter_settings.extend(\n","            self.get_parameter_section(\n","                [(n, p) for n, p in self.encoder.named_parameters()], \n","                lr=CFG.vit_bb_lr, \n","                wd=CFG.vit_bb_wd\n","            )\n","        ) \n","\n","        parameter_settings.extend(\n","            self.get_parameter_section(\n","                [(n, p) for n, p in self.head.named_parameters()], \n","                lr=CFG.hd_lr, \n","                wd=CFG.hd_wd\n","            )\n","        ) \n","\n","        return parameter_settings\n","\n","    def get_parameter_section(self, parameters, lr=None, wd=None): \n","        parameter_settings = []\n","\n","\n","        lr_is_dict = isinstance(lr, dict)\n","        wd_is_dict = isinstance(wd, dict)\n","\n","        layer_no = None\n","        for no, (n,p) in enumerate(parameters):\n","            \n","            for split in n.split('.'):\n","                if split.isnumeric():\n","                    layer_no = int(split)\n","            \n","            if not layer_no:\n","                layer_no = 0\n","            \n","            if lr_is_dict:\n","                for k,v in lr.items():\n","                    if layer_no < int(k):\n","                        temp_lr = v\n","                        break\n","            else:\n","                temp_lr = lr\n","\n","            if wd_is_dict:\n","                for k,v in wd.items():\n","                    if layer_no < int(k):\n","                        temp_wd = v\n","                        break\n","            else:\n","                temp_wd = wd\n","\n","            weight_decay = 0.0 if 'bias' in n else temp_wd\n","\n","            parameter_setting = {\"params\" : p, \"lr\" : temp_lr, \"weight_decay\" : temp_wd}\n","\n","            parameter_settings.append(parameter_setting)\n","\n","            #print(f'no {no} | params {n} | lr {temp_lr} | weight_decay {weight_decay} | requires_grad {p.requires_grad}')\n","\n","        return parameter_settings"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def ArcFace_criterion(logits_m, target, margins):\n","    arc = utilities.ArcFaceLossAdaptiveMargin(margins=margins, s=CFG.s, crit=CFG.crit)\n","    loss_m = arc(logits_m, target, CFG.n_classes)\n","    return loss_m"]},{"cell_type":"markdown","metadata":{},"source":["## Training and Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def train(model, train_loader, optimizer, scaler, scheduler, epoch):\n","    model.train()\n","    loss_metrics = utilities.AverageMeter()\n","    criterion = ArcFace_criterion\n","\n","    tmp = np.sqrt(1 / np.sqrt(value_counts))\n","    margins = (tmp - tmp.min()) / (tmp.max() - tmp.min()) * CFG.m + CFG.m_min\n","        \n","    bar = tqdm(train_loader)\n","    for step, data in enumerate(bar):\n","        step += 1\n","        images = data['images'].to(CFG.device, dtype=torch.float)\n","        labels = data['labels'].to(CFG.device)\n","        batch_size = labels.size(0)\n","\n","        with torch.cuda.amp.autocast(enabled=CFG.autocast):\n","            outputs, features = model(images)\n","\n","        loss = criterion(outputs, labels, margins)\n","        loss_metrics.update(loss.item(), batch_size)\n","        loss = loss / CFG.acc_steps\n","        scaler.scale(loss).backward()\n","\n","        if step % CFG.acc_steps == 0 or step == len(bar):\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","            CFG.global_step += 1\n","                        \n","        lrs = utilities.get_lr_groups(optimizer.param_groups)\n","\n","        loss_avg = loss_metrics.avg\n","\n","        bar.set_postfix(loss=loss_avg, epoch=epoch, lrs=lrs, step=CFG.global_step)\n","    \n","@torch.no_grad()\n","def val(model, valid_loader):\n","    model.eval() \n","\n","    all_embeddings = []\n","    all_labels = [] \n","\n","    for data in tqdm(valid_loader):\n","        images = data['images'].to(CFG.device, dtype=torch.float)\n","        labels = data['labels'].to(CFG.device)\n","\n","        _, embeddings = model(images)\n","\n","        all_embeddings.append(embeddings.detach().cpu().numpy())\n","        all_labels.append(labels.detach().cpu().numpy())\n","\n","\n","    all_embeddings = np.concatenate(all_embeddings, axis=0)\n","    all_labels = np.concatenate(all_labels, axis=0)\n","\n","    return all_embeddings, all_labels\n","\n","def training(train_loader, \n","             gallery_loader, \n","             query_loader, \n","             experiment_folder, \n","             version='v1', \n","             k=3, \n","             reduce_lr_on_epoch=1,\n","             use_rampup=True):\n","    \n","    os.makedirs(experiment_folder, exist_ok=True)\n","    \n","    backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, CFG.model_data)\n","    \n","    model = Model(backbone, CFG.hidden_layer, version, k).to(CFG.device)\n","    \n","#     model.half()\n","    \n","    optimizer = torch.optim.AdamW(model.get_parameters())\n"," \n","    scaler = torch.cuda.amp.GradScaler(enabled=CFG.autocast)\n","\n","    steps_per_epoch = math.ceil(len(train_loader) / CFG.acc_steps)\n","\n","    num_training_steps = math.ceil(CFG.n_epochs * steps_per_epoch)\n","    \n","    if use_rampup:\n","        scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                                    num_training_steps=num_training_steps,\n","                                                    num_warmup_steps=CFG.n_warmup_steps)  \n","    else:\n","        scheduler = get_constant_schedule(optimizer)\n","        \n","    best_score = 0\n","    best_updated_ = 0\n","    CFG.global_step = 0                   \n","    for epoch in range(math.ceil(CFG.n_epochs)):\n","        print(f'starting epoch {epoch}')\n","\n","        # train of product-10k\n","        train(model, train_loader, optimizer, scaler, scheduler, epoch)\n","\n","        # aicrowd test data\n","        print('gallery embeddings')\n","        embeddings_gallery, labels_gallery = val(model, gallery_loader)\n","        print('query embeddings')\n","        embeddings_query, labels_query = val(model, query_loader)\n","\n","        # idk why it is needed\n","        gc.collect()\n","        torch.cuda.empty_cache() \n","\n","        # calculate validation score\n","        _, indices = utilities.get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n","\n","\n","        indices = indices.tolist()\n","        labels_gallery = labels_gallery.tolist()\n","        labels_query = labels_query.tolist()\n","\n","        preds = utilities.convert_indices_to_labels(indices, labels_gallery)\n","        score = utilities.map_per_set(labels_query, preds)\n","        print('validation score', score)\n","\n","        # save model\n","        torch.save({\n","                'model_state_dict': model.encoder.state_dict(),\n","                }, f'{experiment_folder}/model_epoch_{epoch+1}_mAP3_{score:.2f}.pt')\n","\n","        # early stopping\n","        if score > best_score:\n","            best_updated_ = 0\n","            best_score = score\n","\n","        best_updated_ += 1\n","\n","        if best_updated_ >= 3:\n","            print('no improvement done training....')\n","            return model\n","            \n","        if (epoch + 1) % reduce_lr_on_epoch == 0:\n","            scheduler.base_lrs = [g['lr'] * CFG.reduce_lr for g in optimizer.param_groups]\n","            \n","        # to speed up the training\n","        if epoch > 3:\n","            return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, CFG.model_data)\n","\n","# model = Model(backbone, CFG.hidden_layer, 1, 3).to(CFG.device)\n","\n","# embeddings_gallery, labels_gallery = val(model, gallery_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loaders for Training and Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def read_img(img_path, is_gray=False):\n","    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n","    img = cv2.imread(img_path, mode)\n","    if not is_gray:\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    return img\n","\n","def get_final_transform():  \n","    final_transform = T.Compose([\n","            T.Resize(\n","                size=(CFG.image_size, CFG.image_size), \n","                interpolation=T.InterpolationMode.BICUBIC,\n","                antialias=True),\n","            T.ToTensor(), \n","            T.Normalize(\n","                mean=(0.48145466, 0.4578275, 0.40821073), \n","                std=(0.26862954, 0.26130258, 0.27577711)\n","            )\n","        ])\n","    return final_transform\n","\n","class ProductDataset(Dataset):\n","    def __init__(self, \n","                 data, \n","                 transform=None, \n","                 final_transform=None):\n","        self.data = data\n","        self.transform = transform\n","        self.final_transform = final_transform\n","            \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","       \n","        img = read_img(self.data[idx][0])            \n","        \n","        if self.transform is not None:\n","            if isinstance(self.transform, A.Compose):\n","                img = self.transform(image=img)['image']\n","            else:\n","                img = self.transform(img)\n","        \n","        if self.final_transform is not None:\n","            if isinstance(img, np.ndarray):\n","                img =  Image.fromarray(img)\n","            img = self.final_transform(img)\n","            \n","        product_id = self.data[idx][1]\n","        return {\"images\": img, \"labels\": product_id}\n","    \n","def get_product_10k_dataloader(data_train, data_aug='image_net'):\n","    \n","    transform = None\n","    if data_aug == 'image_net':\n","        transform = T.Compose([\n","            T.ToPILImage(),\n","            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET)\n","        ])\n","        \n","    elif data_aug == 'aug_mix':\n","        transform = T.Compose([\n","            T.ToPILImage(),\n","            T.AugMix()\n","        ])\n","    elif data_aug == 'happy_whale':\n","        aug8p3 = A.OneOf([\n","            A.Sharpen(p=0.3),\n","            A.ToGray(p=0.3),\n","            A.CLAHE(p=0.3),\n","        ], p=0.5)\n","\n","        transform = A.Compose([\n","            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n","            A.Resize(CFG.image_size, CFG.image_size),\n","            aug8p3,\n","            A.HorizontalFlip(p=0.5),\n","            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1)\n","        ])\n","    \n","    elif data_aug == 'cut_out':        \n","        transform = A.Compose([\n","            A.HorizontalFlip(p=0.5),\n","            A.ImageCompression(quality_lower=99, quality_upper=100),\n","            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n","            A.Resize(CFG.image_size, CFG.image_size),\n","            A.Cutout(max_h_size=int(CFG.image_size * 0.4), \n","                     max_w_size=int(CFG.image_size * 0.4), \n","                     num_holes=1, p=0.5),\n","        ])\n","    elif data_aug == 'clip':\n","        transform = T.Compose([\n","            T.ToPILImage(),\n","            T.RandomResizedCrop(\n","                size=(224, 224), \n","                scale=(0.9, 1.0), \n","                ratio=(0.75, 1.3333), \n","                interpolation=T.InterpolationMode.BICUBIC,\n","                antialias=True\n","            )\n","        ])\n","    elif data_aug == 'clip+image_net':\n","        transform = T.Compose([\n","            T.ToPILImage(),\n","            T.AutoAugment(T.AutoAugmentPolicy.IMAGENET),\n","            T.RandomResizedCrop(\n","                size=(224, 224), \n","                scale=(0.9, 1.0), \n","                ratio=(0.75, 1.3333), \n","                interpolation=T.InterpolationMode.BICUBIC,\n","                antialias=True\n","            )\n","        ])\n","    \n","    final_transform = get_final_transform()\n","    train_dataset = ProductDataset(data_train, \n","                                   transform, \n","                                   final_transform)\n","    train_loader = DataLoader(train_dataset, \n","                              batch_size = CFG.train_batch_size, \n","                              num_workers=CFG.workers, \n","                              shuffle=True, \n","                              drop_last=True)\n","    print(f'Training Data -> Dataset Length ({len(train_dataset)})')\n","    return train_loader\n","\n","def aicrowd_data_loader(csv_path, img_dir='/kaggle/input/products-10k/products-10k/development_test_data'):\n","    df_g = pd.read_csv(csv_path)\n","    df_g_ = df_g[['img_path', 'product_id']]\n","    df_g_['img_path'] = df_g_.apply(lambda x: img_dir + '/' + x['img_path'], axis=1)\n","    data_ = np.array(df_g_).tolist()\n","    \n","    final_transform = get_final_transform()\n","    dataset = ProductDataset(data_, None, final_transform)\n","    data_loader = DataLoader(dataset, \n","                             batch_size = CFG.valid_batch_size, \n","                             num_workers=CFG.workers, \n","                             shuffle=False, \n","                             drop_last=False)\n","    \n","    print(f'{csv_path} -> Dataset Length ({len(dataset)})')\n","    return data_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# aicrowd datasets\n","gallery_loader = aicrowd_data_loader('/kaggle/input/products-10k/products-10k/development_test_data/gallery.csv') \n","query_loader = aicrowd_data_loader('/kaggle/input/products-10k/products-10k/development_test_data/queries.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k = 3  \n","version = 'v2'\n","data_aug = 'image_net'\n","CFG.reduce_lr = 0.1\n","train_loader = get_product_10k_dataloader(data_train, data_aug)\n","experiment_folder = f'my_experiments/{CFG.model_name}-{CFG.model_data}-{str(data_aug)}-{str(version)}-p10k-h&m-Arcface(k={str(k)})-All-Epoch({str(CFG.n_epochs)})-Reduce_LR_0.1'\n","model = training(train_loader, \n","         gallery_loader, \n","         query_loader, \n","         experiment_folder, \n","         version=version,\n","         k=k)\n","# idk why it is needed\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# backbone, _, _ = open_clip.create_model_and_transforms(CFG.model_name, CFG.model_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# backbone.visual"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import shutil\n","\n","# # Define the folder to zip\n","# folder_to_zip = \"/kaggle/working/my_experiments/ViT-L-14-laion2b_s32b_b82k-image_net-v2-p10k-h&m-Arcface(k=3)-All-Epoch(10)-Reduce_LR_0.1\"\n","\n","# # Define the output zip file name\n","# output_zip_file = \"/kaggle/working/my_folder.zip\"\n","\n","# # Create the zip file\n","# shutil.make_archive(output_zip_file, 'zip', folder_to_zip)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from IPython.display import FileLink\n","\n","# FileLink(r'/kaggle/working/my_folder.zip')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# k = 3  \n","# version = 'v2'\n","# data_aug = 'happy_whale'\n","# CFG.reduce_lr = 0.1\n","# train_loader = get_product_10k_dataloader(data_train, data_aug)\n","# experiment_folder = f'my_experiments/{CFG.model_name}-{CFG.model_data}-{str(data_aug)}-{str(version)}-p10k-h&m-Arcface(k={str(k)})-All-Epoch({str(CFG.n_epochs)})-Reduce_LR_0.1'\n","# training(train_loader, \n","#          gallery_loader, \n","#          query_loader, \n","#          experiment_folder, \n","#          version=version,\n","#          k=k)\n","# # idk why it is needed\n","# gc.collect() \n","# torch.cuda.empty_cache() "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# print(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#     if config.train.classes.loss.name == \"arcface\":\n","#         R = regularizers.RegularFaceRegularizer()\n","#         class_loss = losses.SubCenterArcFaceLoss(\n","#             num_classes=config.dataset.num_of_classes,\n","#             embedding_size=config.model.embedding_dim,\n","#             margin=config.train.classes.loss.arcface.m,\n","#             scale=config.train.classes.loss.arcface.s,\n","#             sub_centers=config.train.classes.loss.arcface.sub_centers,\n","#             weight_regularizer=R,\n","#         ).to(device)\n","\n","#     elif config.train.classes.loss.name == \"triplet_margin\":\n","#         class_loss = losses.TripletMarginLoss(\n","#             margin=config.train.classes.loss.triplet_margin.margin,\n","#             swap=config.train.classes.loss.triplet_margin.swap,\n","#             smooth_loss=config.train.classes.loss.triplet_margin.smooth_loss,\n","#             triplets_per_anchor=\"all\",\n","#             distance=distance,\n","#         )\n","\n","#     elif config.train.classes.loss.name == \"soft_triplet\":\n","\n","#         class_loss = losses.SoftTripleLoss(\n","#             config.dataset.num_of_classes,\n","#             config.model.embedding_dim,\n","#             centers_per_class=config.train.classes.loss.soft_triplet.centers_per_class,\n","#             la=config.train.classes.loss.soft_triplet.la,\n","#             gamma=config.train.classes.loss.soft_triplet.gamma,\n","#             margin=config.train.classes.loss.soft_triplet.margin,\n","#             distance=distance,\n","#             # weight_regularizer=R\n","#         ).to(device)\n","\n","#     elif config.train.classes.loss.name == \"proxy_anchor\":\n","#         class_loss = losses.ProxyAnchorLoss(\n","#             config.dataset.num_of_classes,\n","#             config.model.embedding_dim,\n","#             margin=config.train.classes.loss.proxy_anchor.margin,\n","#             alpha=config.train.classes.loss.proxy_anchor.alpha,\n","#             distance=distance,\n","#         ).to(accelerator.device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
