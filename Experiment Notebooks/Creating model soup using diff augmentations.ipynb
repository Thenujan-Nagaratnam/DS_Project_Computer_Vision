{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# #Setup Gdrive file download extention\n!pip install gdown open-clip-torch faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:44:51.794502Z","iopub.execute_input":"2023-10-30T11:44:51.794932Z","iopub.status.idle":"2023-10-30T11:45:06.792577Z","shell.execute_reply.started":"2023-10-30T11:44:51.794898Z","shell.execute_reply":"2023-10-30T11:45:06.791382Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: open-clip-torch in /opt/conda/lib/python3.10/site-packages (2.23.0)\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.15.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (2023.6.3)\nRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (6.1.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.16.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.1.99)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (3.20.3)\nRequirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.9.7)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (3.1.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->open-clip-torch) (0.2.6)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (2023.9.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.7.22)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open-clip-torch) (0.3.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open-clip-torch) (1.23.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open-clip-torch) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open-clip-torch) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open-clip-torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open-clip-torch) (1.3.0)\nInstalling collected packages: faiss-gpu, gdown\nSuccessfully installed faiss-gpu-1.7.2 gdown-4.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 1BFAJfzzeaUGsPoYELS86HIutJ43D-vat","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:06.794793Z","iopub.execute_input":"2023-10-30T11:45:06.795128Z","iopub.status.idle":"2023-10-30T11:45:22.243076Z","shell.execute_reply.started":"2023-10-30T11:45:06.795100Z","shell.execute_reply":"2023-10-30T11:45:22.242068Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1BFAJfzzeaUGsPoYELS86HIutJ43D-vat\nFrom (redirected): https://drive.google.com/uc?id=1BFAJfzzeaUGsPoYELS86HIutJ43D-vat&confirm=t&uuid=5b3d3333-10eb-4db9-9ae5-b6a804957041\nTo: /kaggle/working/model_weights.pt\n100%|██████████████████████████████████████| 1.26G/1.26G [00:12<00:00, 97.8MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport math\n\nimport numpy as np\n \nimport torch as th\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport open_clip\n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nimport torchvision.transforms as T\n\nfrom tqdm import tqdm\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport time\nimport faiss\nimport copy\nimport argparse","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:22.244483Z","iopub.execute_input":"2023-10-30T11:45:22.244794Z","iopub.status.idle":"2023-10-30T11:45:23.043659Z","shell.execute_reply.started":"2023-10-30T11:45:22.244768Z","shell.execute_reply":"2023-10-30T11:45:23.042907Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def compute_precision_at_k(ranked_targets: np.ndarray,\n                           k: int) -> float:\n\n    \"\"\"\n    Computes the precision at k.\n    Args:\n        ranked_targets: A boolean array of retrieved targets, True if relevant and False otherwise.\n        k: The number of examples to consider\n\n    Returns: The precision at k\n    \"\"\"\n    assert k >= 1\n    assert ranked_targets.size >= k, ValueError('Relevance score length < k')\n    return np.mean(ranked_targets[:k])\n\ndef compute_average_precision(ranked_targets: np.ndarray,\n                              gtp: int) -> float:\n    \n        \n    \"\"\"\n    Computes the average precision.\n    Args:\n        ranked_targets: A boolean array of retrieved targets, True if relevant and False otherwise.\n        gtp: ground truth positives.\n\n    Returns:\n        The average precision.\n    \"\"\"\n    assert gtp >= 1\n    # compute precision at rank only for positive targets\n    out = [compute_precision_at_k(ranked_targets, k + 1) for k in range(ranked_targets.size) if ranked_targets[k]]\n    if len(out) == 0:\n        # no relevant targets in top1000 results\n        return 0.0\n    else:\n        return np.sum(out) / gtp\n\n\ndef calculate_map(ranked_retrieval_results: np.ndarray,\n                  query_labels: np.ndarray,\n                  gallery_labels: np.ndarray) -> float:\n    \n    global current_retrievals, gpt\n    \n    \"\"\"\n    Calculates the mean average precision.\n    Args:\n        ranked_retrieval_results: A 2D array of ranked retrieval results (shape: n_queries x 1000), because we use\n                                top1000 retrieval results.\n        query_labels: A 1D array of query class labels (shape: n_queries).\n        gallery_labels: A 1D array of gallery class labels (shape: n_gallery_items).\n    Returns:\n        The mean average precision.\n    \"\"\"\n    assert ranked_retrieval_results.ndim == 2\n    assert ranked_retrieval_results.shape[1] == 1000\n\n    class_average_precisions = []\n    current_retrievals = []\n\n    class_ids, class_counts = np.unique(gallery_labels, return_counts=True)\n    class_id2quantity_dict = dict(zip(class_ids, class_counts))\n    for gallery_indices, query_class_id in tqdm(\n                            zip(ranked_retrieval_results, query_labels),\n                            total=len(query_labels)):\n        # Checking that no image is repeated in the retrival results\n        assert len(np.unique(gallery_indices)) == len(gallery_indices), \\\n                    ValueError('Repeated images in retrieval results')\n\n        current_retrieval = gallery_labels[gallery_indices] == query_class_id\n        gpt = class_id2quantity_dict[query_class_id]\n        \n        current_retrievals.append(current_retrieval)\n\n        class_average_precisions.append(\n            compute_average_precision(current_retrieval, gpt)\n        )\n\n    mean_average_precision = np.mean(class_average_precisions)\n    return mean_average_precision","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.045745Z","iopub.execute_input":"2023-10-30T11:45:23.046374Z","iopub.status.idle":"2023-10-30T11:45:23.059853Z","shell.execute_reply.started":"2023-10-30T11:45:23.046348Z","shell.execute_reply":"2023-10-30T11:45:23.058922Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport time\n\ndef calculate_l2_distances(query, gallery):\n    return np.linalg.norm(gallery - query, axis=1)\n\ndef get_k_nearest_neighbors(distances, k):\n    indices = np.argsort(distances)[:k]\n    return indices\n\ndef get_similiarity_l2(embeddings_gallery, embeddings_query, k):\n    print('Processing indices...')\n\n    s = time.time()\n\n    scores = []\n    indices = []\n\n    for query in embeddings_query:\n        distances = calculate_l2_distances(query, embeddings_gallery)\n        nearest_indices = get_k_nearest_neighbors(distances, k)\n        scores.append(distances[nearest_indices])\n        indices.append(nearest_indices)\n\n    e = time.time()\n\n    print(f'Finished processing indices, took {e - s}s')\n    return np.array(scores), np.array(indices)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.061093Z","iopub.execute_input":"2023-10-30T11:45:23.061353Z","iopub.status.idle":"2023-10-30T11:45:23.430600Z","shell.execute_reply.started":"2023-10-30T11:45:23.061330Z","shell.execute_reply":"2023-10-30T11:45:23.429586Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def convert_indices_to_labels(indices, labels):\n    indices_copy = copy.deepcopy(indices)\n    for row in indices_copy:\n        for j in range(len(row)):\n            row[j] = labels[row[j]]\n    return indices_copy","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.431756Z","iopub.execute_input":"2023-10-30T11:45:23.432071Z","iopub.status.idle":"2023-10-30T11:45:23.447736Z","shell.execute_reply.started":"2023-10-30T11:45:23.432038Z","shell.execute_reply":"2023-10-30T11:45:23.446988Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if th.cuda.is_available() else 'cpu';","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.448760Z","iopub.execute_input":"2023-10-30T11:45:23.449095Z","iopub.status.idle":"2023-10-30T11:45:23.463202Z","shell.execute_reply.started":"2023-10-30T11:45:23.449071Z","shell.execute_reply":"2023-10-30T11:45:23.462548Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Reading Dataset","metadata":{}},{"cell_type":"code","source":"def read_image(image_file):\n    img = cv2.imread(\n        image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION\n    )\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if img is None:\n        raise ValueError('Failed to read {}'.format(image_file))\n    return img\n\nclass SubmissionDataset(Dataset):\n    def __init__(self, root, annotation_file, transforms, with_bbox=False):\n        self.root = root\n        self.imlist = pd.read_csv(annotation_file)\n        self.transforms = transforms\n        self.with_bbox = with_bbox\n\n    def __getitem__(self, index):\n        cv2.setNumThreads(6)\n\n        full_imname = os.path.join(self.root, self.imlist['img_path'][index])\n        img = read_image(full_imname)\n\n        if self.with_bbox:\n            x, y, w, h = self.imlist.loc[index, 'bbox_x':'bbox_h']\n            img = img[y:y+h, x:x+w, :]\n\n        img = Image.fromarray(img)\n        img = self.transforms(img)\n        product_id = self.imlist['product_id'][index]\n        return img, product_id\n\n    def __len__(self):\n        return len(self.imlist)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.464139Z","iopub.execute_input":"2023-10-30T11:45:23.464376Z","iopub.status.idle":"2023-10-30T11:45:23.478687Z","shell.execute_reply.started":"2023-10-30T11:45:23.464355Z","shell.execute_reply":"2023-10-30T11:45:23.478013Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_transform():  \n    transform = T.Compose([\n            T.Resize(\n                size=(224, 224), \n                interpolation=T.InterpolationMode.BICUBIC,\n                antialias=True),\n            T.ToTensor(), \n            T.Normalize(\n                mean=(0.48145466, 0.4578275, 0.40821073), \n                std=(0.26862954, 0.26130258, 0.27577711)\n            )\n        ])\n    return transform\n\n@th.no_grad()\ndef get_feature_vector(model, dataloader, use_cuda=True):\n    features = []\n    product_id = []\n    \n    for imgs, p_id in tqdm(dataloader):\n        if use_cuda:\n            imgs = imgs.cuda()\n        features.append(th.squeeze(model(imgs.half())).detach().cpu().numpy().astype(np.float32))\n        product_id.append(th.squeeze(p_id).detach().cpu().numpy())\n\n    return np.concatenate(features, axis=0), np.concatenate(product_id)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.479604Z","iopub.execute_input":"2023-10-30T11:45:23.479864Z","iopub.status.idle":"2023-10-30T11:45:23.497502Z","shell.execute_reply.started":"2023-10-30T11:45:23.479841Z","shell.execute_reply":"2023-10-30T11:45:23.496750Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Getting the CLIP model's embedding","metadata":{}},{"cell_type":"code","source":"# weights_path_large = '/kaggle/input/vit-l-14-0-52/vit-l-14-1-0.52.pt'\n\n# vit_backbone = open_clip.create_model_and_transforms('ViT-L-14', None)[0].visual\n# vit_backbone.load_state_dict(th.load(weights_path_large)['model_state_dict'])\n# vit_backbone.half()   # Apply half precision to the backbone model\n# vit_backbone.eval()   # Dropping unecessary layers\n# model = vit_backbone\n# model.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:23.501143Z","iopub.execute_input":"2023-10-30T11:45:23.501396Z","iopub.status.idle":"2023-10-30T11:45:23.508418Z","shell.execute_reply.started":"2023-10-30T11:45:23.501375Z","shell.execute_reply":"2023-10-30T11:45:23.507720Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"weights_path_huge = '/kaggle/working/convnext_large_d_320-soup.pt'\n\nvit_backbone = open_clip.create_model_and_transforms('convnext_large_d_320', None)[0].visual\nvit_backbone.load_state_dict(th.load(weights_path_huge))\nvit_backbone.half()   # Apply half precision to the backbone model\nvit_backbone.eval()   # Dropping unecessary layers\nmodel = vit_backbone\nmodel.cuda();","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:34.715104Z","iopub.execute_input":"2023-10-30T11:46:34.715773Z","iopub.status.idle":"2023-10-30T11:46:40.723220Z","shell.execute_reply.started":"2023-10-30T11:46:34.715742Z","shell.execute_reply":"2023-10-30T11:46:40.722373Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# import torch\n# # Specify the path for the binary file\n# binary_file_path = \"/kaggle/working/model.bin\"\n\n# # Save the model's state dictionary to the binary file\n# torch.save(model, binary_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:43.323160Z","iopub.execute_input":"2023-10-30T11:46:43.323547Z","iopub.status.idle":"2023-10-30T11:46:43.327915Z","shell.execute_reply.started":"2023-10-30T11:46:43.323517Z","shell.execute_reply":"2023-10-30T11:46:43.327061Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# import zipfile\n# import os\n\n# def zip_folder(folder_path, zip_filename):\n#     with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n#         for root, _, files in os.walk(folder_path):\n#             for file in files:\n#                 file_path = os.path.join(root, file)\n#                 arcname = os.path.relpath(file_path, folder_path)\n#                 zipf.write(file_path, arcname)\n\n# # Replace 'your_folder_path' with the actual path to the folder you want to zip\n# folder_to_zip = '/kaggle/working'\n# output_zip_path = 'VIT-H-14.zip'\n\n# zip_folder(folder_to_zip, output_zip_path)\n\n# from IPython.display import FileLink\n\n# # Display a download link for the zip file\n# FileLink(output_zip_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:43.519287Z","iopub.execute_input":"2023-10-30T11:46:43.519570Z","iopub.status.idle":"2023-10-30T11:46:43.524143Z","shell.execute_reply.started":"2023-10-30T11:46:43.519546Z","shell.execute_reply":"2023-10-30T11:46:43.523228Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# !rm model1.pt","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:43.704193Z","iopub.execute_input":"2023-10-30T11:46:43.704577Z","iopub.status.idle":"2023-10-30T11:46:43.708698Z","shell.execute_reply.started":"2023-10-30T11:46:43.704547Z","shell.execute_reply":"2023-10-30T11:46:43.707821Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"transform = get_transform()\n\nimg_dir = \"/kaggle/input/vprtestdata/public_dataset/\"\n\ndataset_train = SubmissionDataset(img_dir, os.path.join(img_dir, \"gallery.csv\"), transform)\ndataloader_train = DataLoader(dataset_train, batch_size=512, num_workers=4)\ndataset_test = SubmissionDataset(img_dir, os.path.join(img_dir, \"queries.csv\"), transform, with_bbox=True)\ndataloader_test = DataLoader(dataset_test, batch_size=512, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:43.958428Z","iopub.execute_input":"2023-10-30T11:46:43.958789Z","iopub.status.idle":"2023-10-30T11:46:44.011164Z","shell.execute_reply.started":"2023-10-30T11:46:43.958758Z","shell.execute_reply":"2023-10-30T11:46:44.010349Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def predict(model):\n    global feature_vectors_gallery, labels_gallery\n    \n    feature_vectors_gallery, labels_gallery = get_feature_vector(model, dataloader_train, 1)\n    feature_vectors_query, labels_query = get_feature_vector(model, dataloader_test, 1)\n    \n    scores, indices = get_similiarity_l2(feature_vectors_gallery, feature_vectors_query, 1000)\n\n    indices = indices.tolist()\n    labels_gallery = labels_gallery.tolist()\n    labels_query = labels_query.tolist()\n\n    return indices","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:44.467220Z","iopub.execute_input":"2023-10-30T11:46:44.467567Z","iopub.status.idle":"2023-10-30T11:46:44.473200Z","shell.execute_reply.started":"2023-10-30T11:46:44.467542Z","shell.execute_reply":"2023-10-30T11:46:44.472286Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"preds = predict(model)\n\npreds_df = pd.DataFrame(preds)\npreds_df.to_csv('preds.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:46:45.045974Z","iopub.execute_input":"2023-10-30T11:46:45.046388Z","iopub.status.idle":"2023-10-30T11:48:03.645385Z","shell.execute_reply.started":"2023-10-30T11:46:45.046359Z","shell.execute_reply":"2023-10-30T11:48:03.644384Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"100%|██████████| 3/3 [00:35<00:00, 11.93s/it]\n100%|██████████| 4/4 [00:38<00:00,  9.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Processing indices...\nFinished processing indices, took 2.4535932540893555s\n","output_type":"stream"}]},{"cell_type":"code","source":"preds_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:48:03.647811Z","iopub.execute_input":"2023-10-30T11:48:03.648171Z","iopub.status.idle":"2023-10-30T11:48:03.674897Z","shell.execute_reply.started":"2023-10-30T11:48:03.648143Z","shell.execute_reply":"2023-10-30T11:48:03.673952Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"   0    1    2     3    4     5    6     7    8    9    ...  990   991  992  \\\n0  768  942  335   923  193   285  266   714  136  507  ...  759   483  791   \n1    8  120  426   196  991   214  927   689  950  275  ...  409   809  287   \n2  494  205  734  1041  829   631  201  1046  946  375  ...  208  1057  573   \n3  384  322  778   185  824  1036  416   641  500  280  ...  759   468  823   \n4  363  286  589   294  997   692  975   562  387  868  ...  105   613  253   \n\n   993  994   995   996  997  998  999  \n0  663  690   800   905   52  130  529  \n1  704  428   101   951  471  757    1  \n2  626  981   429   457  156  206  673  \n3  344  965  1018   423  443  289  621  \n4  175  491  1022  1027   19  350  118  \n\n[5 rows x 1000 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>990</th>\n      <th>991</th>\n      <th>992</th>\n      <th>993</th>\n      <th>994</th>\n      <th>995</th>\n      <th>996</th>\n      <th>997</th>\n      <th>998</th>\n      <th>999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>768</td>\n      <td>942</td>\n      <td>335</td>\n      <td>923</td>\n      <td>193</td>\n      <td>285</td>\n      <td>266</td>\n      <td>714</td>\n      <td>136</td>\n      <td>507</td>\n      <td>...</td>\n      <td>759</td>\n      <td>483</td>\n      <td>791</td>\n      <td>663</td>\n      <td>690</td>\n      <td>800</td>\n      <td>905</td>\n      <td>52</td>\n      <td>130</td>\n      <td>529</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8</td>\n      <td>120</td>\n      <td>426</td>\n      <td>196</td>\n      <td>991</td>\n      <td>214</td>\n      <td>927</td>\n      <td>689</td>\n      <td>950</td>\n      <td>275</td>\n      <td>...</td>\n      <td>409</td>\n      <td>809</td>\n      <td>287</td>\n      <td>704</td>\n      <td>428</td>\n      <td>101</td>\n      <td>951</td>\n      <td>471</td>\n      <td>757</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>494</td>\n      <td>205</td>\n      <td>734</td>\n      <td>1041</td>\n      <td>829</td>\n      <td>631</td>\n      <td>201</td>\n      <td>1046</td>\n      <td>946</td>\n      <td>375</td>\n      <td>...</td>\n      <td>208</td>\n      <td>1057</td>\n      <td>573</td>\n      <td>626</td>\n      <td>981</td>\n      <td>429</td>\n      <td>457</td>\n      <td>156</td>\n      <td>206</td>\n      <td>673</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>384</td>\n      <td>322</td>\n      <td>778</td>\n      <td>185</td>\n      <td>824</td>\n      <td>1036</td>\n      <td>416</td>\n      <td>641</td>\n      <td>500</td>\n      <td>280</td>\n      <td>...</td>\n      <td>759</td>\n      <td>468</td>\n      <td>823</td>\n      <td>344</td>\n      <td>965</td>\n      <td>1018</td>\n      <td>423</td>\n      <td>443</td>\n      <td>289</td>\n      <td>621</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>363</td>\n      <td>286</td>\n      <td>589</td>\n      <td>294</td>\n      <td>997</td>\n      <td>692</td>\n      <td>975</td>\n      <td>562</td>\n      <td>387</td>\n      <td>868</td>\n      <td>...</td>\n      <td>105</td>\n      <td>613</td>\n      <td>253</td>\n      <td>175</td>\n      <td>491</td>\n      <td>1022</td>\n      <td>1027</td>\n      <td>19</td>\n      <td>350</td>\n      <td>118</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1000 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"seller_gt = pd.read_csv('/kaggle/input/vprtestdata/public_dataset/gallery.csv')\ngallery_labels = seller_gt['product_id'].values\nuser_gt = pd.read_csv('/kaggle/input/vprtestdata/public_dataset/queries.csv')\nquery_labels = user_gt['product_id'].values\n\n# Evalaute metrics\nprint(\"Evaluation Results\")\nresults = {\"mAP\": calculate_map(np.array(preds), query_labels, gallery_labels)}\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:48:03.676830Z","iopub.execute_input":"2023-10-30T11:48:03.677219Z","iopub.status.idle":"2023-10-30T11:48:04.480582Z","shell.execute_reply.started":"2023-10-30T11:48:03.677187Z","shell.execute_reply":"2023-10-30T11:48:04.479580Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Evaluation Results\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1935/1935 [00:00<00:00, 3702.30it/s]","output_type":"stream"},{"name":"stdout","text":"{'mAP': 0.49652107022765996}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# sample image similarity search","metadata":{}},{"cell_type":"code","source":"len(current_retrievals)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.452182Z","iopub.status.idle":"2023-10-30T11:45:35.452512Z","shell.execute_reply.started":"2023-10-30T11:45:35.452352Z","shell.execute_reply":"2023-10-30T11:45:35.452368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_img(image):\n    img = image\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    if isinstance(img, np.ndarray):\n        img =  Image.fromarray(img)\n        \n    img = transform(img)\n\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.454217Z","iopub.status.idle":"2023-10-30T11:45:35.454553Z","shell.execute_reply.started":"2023-10-30T11:45:35.454394Z","shell.execute_reply":"2023-10-30T11:45:35.454409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@th.no_grad()\ndef get_feature_vector_img(model, imgs, use_cuda=True):\n    features = []\n    if use_cuda:\n        imgs = imgs.cuda()\n    x = (model(imgs.half())).detach().cpu().numpy().astype(np.float32)  # .half()\n    print(model(imgs.half()).shape)\n    features.append(x)\n\n    return np.concatenate(features, axis=0)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.456432Z","iopub.status.idle":"2023-10-30T11:45:35.456781Z","shell.execute_reply.started":"2023-10-30T11:45:35.456611Z","shell.execute_reply":"2023-10-30T11:45:35.456627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = '/kaggle/input/vprtestdata/public_dataset/queries/accelerated-glorious-fennec-of-reward.jpg'\n\ndef get_similar_prods(img_path):\n\n    image = read_image(img_path)\n    image = transform_img(image)\n    image = image.unsqueeze(dim=0)\n#     feature_vectors_gallery, labels_gallery = get_feature_vector(model, dataloader_train, 1)\n    feature_vectors_query = get_feature_vector_img(model, image, 1)\n    scores, indices = get_similiarity_l2(feature_vectors_gallery, feature_vectors_query, 1000)\n    preds = convert_indices_to_labels(indices, labels_gallery)\n    indices = indices.tolist()\n\n    return [indices , preds]\n    \n[similar_images, labels] = get_similar_prods(img_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.457808Z","iopub.status.idle":"2023-10-30T11:45:35.458198Z","shell.execute_reply.started":"2023-10-30T11:45:35.458000Z","shell.execute_reply":"2023-10-30T11:45:35.458039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"csv_path_q = '/kaggle/input/vprtestdata/public_dataset/queries.csv'  \ndata_q = pd.read_csv(csv_path_q)\n\nx = data_q[data_q['img_path'] == 'queries/accelerated-glorious-fennec-of-reward.jpg']\n\nx","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.459582Z","iopub.status.idle":"2023-10-30T11:45:35.459971Z","shell.execute_reply.started":"2023-10-30T11:45:35.459762Z","shell.execute_reply":"2023-10-30T11:45:35.459779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_df1 = pd.DataFrame(similar_images)\n\npreds_df1","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.460952Z","iopub.status.idle":"2023-10-30T11:45:35.461287Z","shell.execute_reply.started":"2023-10-30T11:45:35.461132Z","shell.execute_reply":"2023-10-30T11:45:35.461147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\nimg = Image.open(img_path)\n\nimg = img.resize((224, 224))\n\nplt.figure(figsize=(4, 4))\nplt.imshow(img)\nplt.axis('off')\nplt.savefig('query.png')  \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.462069Z","iopub.status.idle":"2023-10-30T11:45:35.462374Z","shell.execute_reply.started":"2023-10-30T11:45:35.462222Z","shell.execute_reply":"2023-10-30T11:45:35.462236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ncsv_path = '/kaggle/input/vprtestdata/public_dataset/gallery.csv'  \ndata = pd.read_csv(csv_path)\n\nprod_ids = similar_images[0][:100]  \n\nnum_images = len(prod_ids)\nnum_columns = 10\nnum_rows = (num_images + num_columns - 1) // num_columns\n\nfig, axes = plt.subplots(num_rows, num_columns, figsize=(15, 15))\n\nfor i, prod_id in enumerate(prod_ids):\n    row = data[data['seller_img_id'] == prod_id]\n    \n    if not row.empty:\n        image_path = '/kaggle/input/vprtestdata/public_dataset/' + row.iloc[0]['img_path']\n        img = Image.open(image_path)\n        \n        img = img.resize((224, 224))\n        \n        row_idx = i // num_columns\n        col_idx = i % num_columns\n        \n        ax = axes[row_idx, col_idx]\n        ax.imshow(img)\n        ax.set_title(f\"Image ID: {prod_id}\")\n        ax.axis('off')\n\nfor i in range(num_images, num_rows * num_columns):\n    fig.delaxes(axes.flatten()[i])\n\nplt.tight_layout()\n\nplt.savefig('inference.png')  \n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.463761Z","iopub.status.idle":"2023-10-30T11:45:35.464122Z","shell.execute_reply.started":"2023-10-30T11:45:35.463929Z","shell.execute_reply":"2023-10-30T11:45:35.463944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")","metadata":{"execution":{"iopub.status.busy":"2023-10-30T11:45:35.465722Z","iopub.status.idle":"2023-10-30T11:45:35.466084Z","shell.execute_reply.started":"2023-10-30T11:45:35.465897Z","shell.execute_reply":"2023-10-30T11:45:35.465913Z"},"trusted":true},"execution_count":null,"outputs":[]}]}