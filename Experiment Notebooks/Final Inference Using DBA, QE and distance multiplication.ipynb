{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efda4f0b",
   "metadata": {
    "papermill": {
     "duration": 0.006172,
     "end_time": "2023-11-01T19:13:59.905671",
     "exception": false,
     "start_time": "2023-11-01T19:13:59.899499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MODELS TO BE ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6827f437",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:13:59.918799Z",
     "iopub.status.busy": "2023-11-01T19:13:59.918012Z",
     "iopub.status.idle": "2023-11-01T19:14:17.429339Z",
     "shell.execute_reply": "2023-11-01T19:14:17.428154Z"
    },
    "papermill": {
     "duration": 17.52074,
     "end_time": "2023-11-01T19:14:17.432190",
     "exception": false,
     "start_time": "2023-11-01T19:13:59.911450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\r\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\r\n",
      "Collecting open-clip-torch\r\n",
      "  Downloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting faiss-gpu\r\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\r\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\r\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (2.0.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.15.1)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (2023.6.3)\r\n",
      "Collecting ftfy (from open-clip-torch)\r\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.16.4)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.1.99)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (3.20.3)\r\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from open-clip-torch) (0.9.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (4.6.3)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (3.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.9.0->open-clip-torch) (3.1.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\r\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->open-clip-torch) (0.2.6)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (2023.6.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (6.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->open-clip-torch) (21.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm->open-clip-torch) (0.3.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->open-clip-torch) (1.23.5)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->open-clip-torch) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub->open-clip-torch) (3.0.9)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.9.0->open-clip-torch) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.9.0->open-clip-torch) (1.3.0)\r\n",
      "Installing collected packages: faiss-gpu, ftfy, gdown, open-clip-torch\r\n",
      "Successfully installed faiss-gpu-1.7.2 ftfy-6.1.1 gdown-4.7.1 open-clip-torch-2.23.0\r\n"
     ]
    }
   ],
   "source": [
    "# #Setup Gdrive file download extention \n",
    "!pip install gdown open-clip-torch faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86924f22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:17.452339Z",
     "iopub.status.busy": "2023-11-01T19:14:17.451950Z",
     "iopub.status.idle": "2023-11-01T19:14:19.143561Z",
     "shell.execute_reply": "2023-11-01T19:14:19.142256Z"
    },
    "papermill": {
     "duration": 1.70507,
     "end_time": "2023-11-01T19:14:19.146341",
     "exception": false,
     "start_time": "2023-11-01T19:14:17.441271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  warnings.warn(\r\n",
      "Access denied with the following error:\r\n",
      "\r\n",
      " \tToo many users have viewed or downloaded this file recently. Please\r\n",
      "\ttry accessing the file again later. If the file you are trying to\r\n",
      "\taccess is particularly large or is shared with many people, it may\r\n",
      "\ttake up to 24 hours to be able to view or download the file. If you\r\n",
      "\tstill can't access a file after 24 hours, contact your domain\r\n",
      "\tadministrator. \r\n",
      "\r\n",
      "You may still be able to access the file from the browser:\r\n",
      "\r\n",
      "\t https://drive.google.com/uc?id=1BFAJfzzeaUGsPoYELS86HIutJ43D-vat \r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1BFAJfzzeaUGsPoYELS86HIutJ43D-vat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce11799",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:19.169006Z",
     "iopub.status.busy": "2023-11-01T19:14:19.168646Z",
     "iopub.status.idle": "2023-11-01T19:14:19.173542Z",
     "shell.execute_reply": "2023-11-01T19:14:19.172573Z"
    },
    "papermill": {
     "duration": 0.018685,
     "end_time": "2023-11-01T19:14:19.175691",
     "exception": false,
     "start_time": "2023-11-01T19:14:19.157006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path1 = '/kaggle/working/model1.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d53a27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:19.197709Z",
     "iopub.status.busy": "2023-11-01T19:14:19.196808Z",
     "iopub.status.idle": "2023-11-01T19:14:35.336853Z",
     "shell.execute_reply": "2023-11-01T19:14:35.336042Z"
    },
    "papermill": {
     "duration": 16.153425,
     "end_time": "2023-11-01T19:14:35.339164",
     "exception": false,
     "start_time": "2023-11-01T19:14:19.185739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    " \n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import open_clip\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# from vpr_utils import get_similiarity_l2\n",
    "from vpr_utils import convert_indices_to_labels\n",
    "from vpr_utils import map_per_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24ccfa0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:35.358838Z",
     "iopub.status.busy": "2023-11-01T19:14:35.358560Z",
     "iopub.status.idle": "2023-11-01T19:14:35.469891Z",
     "shell.execute_reply": "2023-11-01T19:14:35.468788Z"
    },
    "papermill": {
     "duration": 0.123396,
     "end_time": "2023-11-01T19:14:35.471914",
     "exception": false,
     "start_time": "2023-11-01T19:14:35.348518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if th.cuda.is_available() else 'cpu';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c208d4d",
   "metadata": {
    "papermill": {
     "duration": 0.008623,
     "end_time": "2023-11-01T19:14:35.489869",
     "exception": false,
     "start_time": "2023-11-01T19:14:35.481246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225db1fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:35.508779Z",
     "iopub.status.busy": "2023-11-01T19:14:35.508505Z",
     "iopub.status.idle": "2023-11-01T19:14:35.521603Z",
     "shell.execute_reply": "2023-11-01T19:14:35.520703Z"
    },
    "papermill": {
     "duration": 0.024794,
     "end_time": "2023-11-01T19:14:35.523538",
     "exception": false,
     "start_time": "2023-11-01T19:14:35.498744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from PIL import Image\n",
    "import copy\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "def average_query_expansion(query_vecs, reference_vecs, top_k=3):\n",
    "    \"\"\"\n",
    "    Average Query Expansion (AQE)\n",
    "    Ondrej Chum, et al. \"Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval,\"\n",
    "    International Conference of Computer Vision. 2007.\n",
    "    https://www.robots.ox.ac.uk/~vgg/publications/papers/chum07b.pdf\n",
    "    \"\"\"\n",
    "    # Query augmentation\n",
    "    sim_mat = torch.cdist(query_vecs, reference_vecs)\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref_mean = torch.mean(reference_vecs[indices[:, :top_k], :], dim=1)\n",
    "    query_vecs = torch.cat([query_vecs, top_k_ref_mean], dim=1)\n",
    "\n",
    "    # Reference augmentation\n",
    "    sim_mat = torch.cdist(reference_vecs, reference_vecs)\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref_mean = torch.mean(reference_vecs[indices[:, 1 : top_k + 1], :], dim=1)\n",
    "    reference_vecs = torch.cat([reference_vecs, top_k_ref_mean], dim=1)\n",
    "\n",
    "    return query_vecs, reference_vecs\n",
    "\n",
    "\n",
    "def db_augmentation(query_vecs, reference_vecs, top_k=3):\n",
    "    \"\"\"\n",
    "    Database-side feature augmentation (DBA)\n",
    "    Albert Gordo, et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval,\"\n",
    "    International Journal of Computer Vision. 2017.\n",
    "    https://link.springer.com/article/10.1007/s11263-017-1016-8\n",
    "    \"\"\"\n",
    "    weights = torch.logspace(0, -2.0, top_k + 1).cuda()\n",
    "\n",
    "    # Query augmentation\n",
    "    sim_mat = torch.cdist(query_vecs, reference_vecs)\n",
    "\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref = reference_vecs[indices[:, :top_k], :]\n",
    "    query_vecs = torch.tensordot(\n",
    "        weights,\n",
    "        torch.cat([torch.unsqueeze(query_vecs, 1), top_k_ref], dim=1),\n",
    "        dims=([0], [1]),\n",
    "    )\n",
    "\n",
    "    # Reference augmentation\n",
    "    sim_mat = torch.cdist(reference_vecs, reference_vecs)\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref = reference_vecs[indices[:, : top_k + 1], :]\n",
    "    reference_vecs = torch.tensordot(weights, top_k_ref, dims=([0], [1]))\n",
    "    # reference_vecs = torch.tensordot(weights, torch.cat([torch.unsqueeze(query_vecs, 1), top_k_ref], dim=1), dims=([0], [1]))\n",
    "\n",
    "    return query_vecs, reference_vecs\n",
    "\n",
    "\n",
    "\n",
    "def calculate_l2_distances(query, gallery):\n",
    "    return np.linalg.norm(gallery - query, axis=1)\n",
    "\n",
    "def get_k_nearest_neighbors(distances, k):\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edc430fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:35.542721Z",
     "iopub.status.busy": "2023-11-01T19:14:35.542249Z",
     "iopub.status.idle": "2023-11-01T19:14:35.550905Z",
     "shell.execute_reply": "2023-11-01T19:14:35.550002Z"
    },
    "papermill": {
     "duration": 0.020213,
     "end_time": "2023-11-01T19:14:35.552722",
     "exception": false,
     "start_time": "2023-11-01T19:14:35.532509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(image_file):\n",
    "    img = cv2.imread(\n",
    "        image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION\n",
    "    )\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if img is None:\n",
    "        raise ValueError('Failed to read {}'.format(image_file))\n",
    "    return img\n",
    "\n",
    "\n",
    "class SubmissionDataset(Dataset):\n",
    "    def __init__(self, root, annotation_file, transforms, with_bbox=False):\n",
    "        self.root = root\n",
    "        self.imlist = pd.read_csv(annotation_file)\n",
    "        self.transforms = transforms\n",
    "        self.with_bbox = with_bbox\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cv2.setNumThreads(6)\n",
    "\n",
    "        full_imname = os.path.join(self.root, self.imlist['img_path'][index])\n",
    "        img = read_image(full_imname)\n",
    "\n",
    "        if self.with_bbox:\n",
    "            x, y, w, h = self.imlist.loc[index, 'bbox_x':'bbox_h']\n",
    "            img = img[y:y+h, x:x+w, :]\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transforms(img)\n",
    "        product_id = self.imlist['product_id'][index]\n",
    "        return img, product_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b3e08c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:14:35.571900Z",
     "iopub.status.busy": "2023-11-01T19:14:35.571612Z",
     "iopub.status.idle": "2023-11-01T19:16:04.583589Z",
     "shell.execute_reply": "2023-11-01T19:16:04.582564Z"
    },
    "papermill": {
     "duration": 89.033934,
     "end_time": "2023-11-01T19:16:04.595512",
     "exception": false,
     "start_time": "2023-11-01T19:14:35.561578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.75410071 0.76879095 0.7930227 ]\n",
      "Std: [0.25881864 0.25062647 0.23544936]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "imgs_path = \"/kaggle/input/products-10k/products-10k/development_test_data/gallery/\"\n",
    "\n",
    "files = [imgs_path + x for x in os.listdir(imgs_path) if \"jpg\" in x]\n",
    "\n",
    "# List of image paths\n",
    "image_paths = files\n",
    "\n",
    "# Calculate mean and std\n",
    "mean = np.zeros(3)\n",
    "std = np.zeros(3)\n",
    "num_images = len(image_paths)\n",
    "\n",
    "for image_path in image_paths:\n",
    "    img = cv2.imread(image_path) / 255.0  # Read and normalize image\n",
    "    mean += img.mean(axis=(0, 1))\n",
    "    std += img.std(axis=(0, 1))\n",
    "\n",
    "mean /= num_images\n",
    "std /= num_images\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Std:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9589e205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:16:04.615080Z",
     "iopub.status.busy": "2023-11-01T19:16:04.614351Z",
     "iopub.status.idle": "2023-11-01T19:16:04.622747Z",
     "shell.execute_reply": "2023-11-01T19:16:04.621851Z"
    },
    "papermill": {
     "duration": 0.020174,
     "end_time": "2023-11-01T19:16:04.624563",
     "exception": false,
     "start_time": "2023-11-01T19:16:04.604389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transform():  \n",
    "    transform = T.Compose([\n",
    "            T.Resize(\n",
    "                size=(224, 224), \n",
    "                interpolation=T.InterpolationMode.BICUBIC,\n",
    "                antialias=True),\n",
    "            T.ToTensor(), \n",
    "            T.Normalize(\n",
    "                mean=(0.48145466, 0.4578275, 0.40821073), \n",
    "                std=(0.26862954, 0.26130258, 0.27577711)\n",
    "            )\n",
    "        ])\n",
    "    return transform\n",
    "\n",
    "@th.no_grad()\n",
    "def get_feature_vector(model, dataloader, epoch=10, use_cuda=True):\n",
    "    features = []\n",
    "    product_id = []\n",
    "    \n",
    "    for imgs, p_id in tqdm(dataloader):\n",
    "        if use_cuda:\n",
    "            imgs = imgs.cuda()\n",
    "        features.append(th.squeeze(model(imgs.half())).detach().cpu().numpy().astype(np.float32))\n",
    "        product_id.append(th.squeeze(p_id).detach().cpu().numpy())\n",
    "\n",
    "\n",
    "    return np.concatenate(features, axis=0), np.concatenate(product_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5128bad",
   "metadata": {
    "papermill": {
     "duration": 0.008701,
     "end_time": "2023-11-01T19:16:04.642487",
     "exception": false,
     "start_time": "2023-11-01T19:16:04.633786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Getting the CLIP model's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba568e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:16:04.661285Z",
     "iopub.status.busy": "2023-11-01T19:16:04.661014Z",
     "iopub.status.idle": "2023-11-01T19:16:32.102406Z",
     "shell.execute_reply": "2023-11-01T19:16:32.101414Z"
    },
    "papermill": {
     "duration": 27.453611,
     "end_time": "2023-11-01T19:16:32.104885",
     "exception": false,
     "start_time": "2023-11-01T19:16:04.651274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = open_clip.create_model_and_transforms('ViT-H-14', None)[0].visual\n",
    "backbone.load_state_dict(th.load('/kaggle/input/final-model-weights-version-01/all-model-weights/vit_h_14_0.607_224_ce.pt'))\n",
    "backbone.half()   # Apply half precision to the backbone model\n",
    "backbone.eval()   # Dropping unecessary layers\n",
    "model1 = backbone\n",
    "model1.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2e24d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:16:32.125061Z",
     "iopub.status.busy": "2023-11-01T19:16:32.124720Z",
     "iopub.status.idle": "2023-11-01T19:17:07.074292Z",
     "shell.execute_reply": "2023-11-01T19:17:07.073501Z"
    },
    "papermill": {
     "duration": 34.962211,
     "end_time": "2023-11-01T19:17:07.076807",
     "exception": false,
     "start_time": "2023-11-01T19:16:32.114596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "backbone = open_clip.create_model_and_transforms('convnext_xxlarge', None)[0].visual\n",
    "backbone.load_state_dict(th.load('/kaggle/input/final-model-weights-version-01/all-model-weights/convnext_xxlarge_0.60_320_ce.pt'))\n",
    "backbone.half()   # Apply half precision to the backbone model\n",
    "backbone.eval();   # Dropping unecessary layers\n",
    "model2 = backbone\n",
    "model2.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08559ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:17:07.096882Z",
     "iopub.status.busy": "2023-11-01T19:17:07.096580Z",
     "iopub.status.idle": "2023-11-01T19:17:07.138600Z",
     "shell.execute_reply": "2023-11-01T19:17:07.137819Z"
    },
    "papermill": {
     "duration": 0.053971,
     "end_time": "2023-11-01T19:17:07.140667",
     "exception": false,
     "start_time": "2023-11-01T19:17:07.086696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = get_transform()\n",
    "\n",
    "img_dir = \"/kaggle/input/products-10k/products-10k/development_test_data\"\n",
    "\n",
    "dataset_train = SubmissionDataset(img_dir, os.path.join(img_dir, \"gallery.csv\"), transform)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=256, num_workers=4)\n",
    "\n",
    "dataset_test = SubmissionDataset(img_dir, os.path.join(img_dir, \"queries.csv\"), transform, with_bbox=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=256, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f069353c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:17:07.161013Z",
     "iopub.status.busy": "2023-11-01T19:17:07.160719Z",
     "iopub.status.idle": "2023-11-01T19:17:07.183601Z",
     "shell.execute_reply": "2023-11-01T19:17:07.182702Z"
    },
    "papermill": {
     "duration": 0.035388,
     "end_time": "2023-11-01T19:17:07.185579",
     "exception": false,
     "start_time": "2023-11-01T19:17:07.150191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from PIL import Image\n",
    "import copy\n",
    "import numpy as np\n",
    "import open_clip\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "def average_query_expansion(query_vecs, reference_vecs, top_k=3):\n",
    "    \"\"\"\n",
    "    Average Query Expansion (AQE)\n",
    "    Ondrej Chum, et al. \"Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval,\"\n",
    "    International Conference of Computer Vision. 2007.\n",
    "    https://www.robots.ox.ac.uk/~vgg/publications/papers/chum07b.pdf\n",
    "    \"\"\"\n",
    "    # Query augmentation\n",
    "    sim_mat = torch.cdist(query_vecs, reference_vecs)\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref_mean = torch.mean(reference_vecs[indices[:, :top_k], :], dim=1)\n",
    "    query_vecs = torch.cat([query_vecs, top_k_ref_mean], dim=1)\n",
    "\n",
    "    # Reference augmentation\n",
    "    sim_mat = torch.cdist(reference_vecs, reference_vecs)\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref_mean = torch.mean(reference_vecs[indices[:, 1 : top_k + 1], :], dim=1)\n",
    "    reference_vecs = torch.cat([reference_vecs, top_k_ref_mean], dim=1)\n",
    "\n",
    "    return query_vecs, reference_vecs\n",
    "\n",
    "\n",
    "def db_augmentation(query_vecs, reference_vecs, top_k=3):\n",
    "    \"\"\"\n",
    "    Database-side feature augmentation (DBA)\n",
    "    Albert Gordo, et al. \"End-to-end Learning of Deep Visual Representations for Image Retrieval,\"\n",
    "    International Journal of Computer Vision. 2017.\n",
    "    https://link.springer.com/article/10.1007/s11263-017-1016-8\n",
    "    \"\"\"\n",
    "    weights = torch.logspace(0, -2.0, top_k + 1).cuda()\n",
    "\n",
    "    # Query augmentation\n",
    "    sim_mat = torch.cdist(query_vecs, reference_vecs)\n",
    "\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref = reference_vecs[indices[:, :top_k], :]\n",
    "    query_vecs = torch.tensordot(\n",
    "        weights,\n",
    "        torch.cat([torch.unsqueeze(query_vecs, 1), top_k_ref], dim=1),\n",
    "        dims=([0], [1]),\n",
    "    )\n",
    "\n",
    "    # Reference augmentation\n",
    "    sim_mat = torch.cdist(reference_vecs, reference_vecs)\n",
    "    indices = torch.argsort(sim_mat, dim=1)\n",
    "\n",
    "    top_k_ref = reference_vecs[indices[:, : top_k + 1], :]\n",
    "    reference_vecs = torch.tensordot(weights, top_k_ref, dims=([0], [1]))\n",
    "    # reference_vecs = torch.tensordot(weights, torch.cat([torch.unsqueeze(query_vecs, 1), top_k_ref], dim=1), dims=([0], [1]))\n",
    "\n",
    "    return query_vecs, reference_vecs\n",
    "\n",
    "\n",
    "\n",
    "def calculate_l2_distances(query, gallery):\n",
    "    return np.linalg.norm(gallery - query, axis=1)\n",
    "\n",
    "def get_k_nearest_neighbors(distances, k):\n",
    "    indices = np.argsort(distances)[:k]\n",
    "    return indices\n",
    "\n",
    "def get_distances(gallery_embeddings, query_embeddings):\n",
    "    print('Processing indices...')\n",
    "    \n",
    "    query_embeddings = torch.from_numpy(query_embeddings).to('cuda')\n",
    "    gallery_embeddings = torch.from_numpy(gallery_embeddings).to('cuda')\n",
    "\n",
    "    concat = torch.cat((query_embeddings, gallery_embeddings), dim=0)\n",
    "    center = torch.mean(concat, dim=0)\n",
    "    query_embeddings = query_embeddings - center\n",
    "    gallery_embeddings = gallery_embeddings - center\n",
    "    gallery_embeddings = torch.nn.functional.normalize(\n",
    "        gallery_embeddings, p=2.0, dim=1\n",
    "    )\n",
    "    query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2.0, dim=1)\n",
    "\n",
    "#     query_embeddings, gallery_embeddings = db_augmentation(\n",
    "#         query_embeddings, gallery_embeddings, top_k=5\n",
    "#     )\n",
    "    \n",
    "#     query_embeddings, gallery_embeddings = average_query_expansion(query_embeddings, gallery_embeddings, top_k=25)\n",
    "\n",
    "\n",
    "    distances = torch.cdist(query_embeddings, gallery_embeddings)\n",
    "    \n",
    "    sorted_distances, sorted_indices = torch.sort(distances, dim=1)\n",
    "\n",
    "    class_ranks = sorted_indices\n",
    "    \n",
    "    # take indexes of the most similar embeddings from the gallery_embeddings\n",
    "    first_gallery_idx = class_ranks[:, 0]\n",
    "    \n",
    "    # take distance value of the most similar embeddings from the gallery_embeddings\n",
    "    first_gallery_dstx = sorted_distances[:, 0]\n",
    "\n",
    "    # take the most similar embedding from the gallery_embeddings by index\n",
    "    rerank_embeddings1 = gallery_embeddings.index_select(0, first_gallery_idx)\n",
    "\n",
    "    # if distance between most similar gallery and query < 0.3 \n",
    "    # then add it to the new embeddings list for ranking (filter_rerank_embeddings1) \n",
    "    # else add embedding from query_embeddings\n",
    "    mask1 = first_gallery_dstx < 0.3\n",
    "    filter_rerank_embeddings1 = torch.where(\n",
    "        mask1.view(-1, 1), rerank_embeddings1, query_embeddings\n",
    "    )\n",
    "    \n",
    "    # averaging and ranking\n",
    "    filter_rerank_embeddings = (\n",
    "        0.5 * filter_rerank_embeddings1 + 0.5 * query_embeddings\n",
    "    )\n",
    "    distances = torch.cdist(filter_rerank_embeddings, gallery_embeddings)\n",
    "    \n",
    "    # then the same thing, but with the two most similar embeddings from gallery_embeddings\n",
    "\n",
    "    sorted_distances, sorted_indices = torch.sort(distances, dim=1)\n",
    "    first_gallery_idx = class_ranks[:, 0]\n",
    "    first_gallery_dstx = sorted_distances[:, 0]\n",
    "    second_gallery_idx = class_ranks[:, 1]\n",
    "    second_gallery_dstx = sorted_distances[:, 1]\n",
    "\n",
    "    rerank_embeddings1 = gallery_embeddings.index_select(0, first_gallery_idx)\n",
    "    rerank_embeddings2 = gallery_embeddings.index_select(0, second_gallery_idx)\n",
    "\n",
    "    mask1 = first_gallery_dstx < 0.3\n",
    "    mask2 = second_gallery_dstx < 0.3\n",
    "\n",
    "    filter_rerank_embeddings1 = torch.where(\n",
    "        mask1.view(-1, 1), rerank_embeddings1, query_embeddings\n",
    "    )\n",
    "    filter_rerank_embeddings2 = torch.where(\n",
    "        mask2.view(-1, 1), rerank_embeddings2, query_embeddings\n",
    "    )\n",
    "\n",
    "    filter_rerank_embeddings = (\n",
    "        0.3 * filter_rerank_embeddings1\n",
    "        + 0.3 * filter_rerank_embeddings2\n",
    "        + 0.4 * query_embeddings\n",
    "    )\n",
    "\n",
    "    distances = torch.cdist(filter_rerank_embeddings, gallery_embeddings)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "    \n",
    "def get_similiarity_l2(distances, k):\n",
    "\n",
    "    sorted_distances = torch.argsort(distances, dim=1)\n",
    "    sorted_distances = sorted_distances.cpu().numpy()[:, :1000]\n",
    "    class_ranks = sorted_distances\n",
    "    \n",
    "    return class_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29d92409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:17:07.204780Z",
     "iopub.status.busy": "2023-11-01T19:17:07.204513Z",
     "iopub.status.idle": "2023-11-01T19:17:07.211495Z",
     "shell.execute_reply": "2023-11-01T19:17:07.210722Z"
    },
    "papermill": {
     "duration": 0.019005,
     "end_time": "2023-11-01T19:17:07.213496",
     "exception": false,
     "start_time": "2023-11-01T19:17:07.194491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model1, model2):\n",
    "    global feature_vectors_gallery\n",
    "    \n",
    "    feature_vectors_gallery, labels_gallery = get_feature_vector(model1, dataloader_train, 1)\n",
    "    feature_vectors_query, labels_query = get_feature_vector(model1, dataloader_test, 1)\n",
    "\n",
    "    distances1 = get_distances(feature_vectors_gallery, feature_vectors_query)\n",
    "\n",
    "    feature_vectors_gallery, labels_gallery = get_feature_vector(model2, dataloader_train, 1)\n",
    "    feature_vectors_query, labels_query = get_feature_vector(model2, dataloader_test, 1)\n",
    "\n",
    "    distances2 = get_distances(feature_vectors_gallery, feature_vectors_query)\n",
    "\n",
    "    distances = torch.mul(0.7*distances1, distances2)\n",
    "\n",
    "    indices = get_similiarity_l2(distances, 1000)\n",
    "\n",
    "    indices = indices.tolist()\n",
    "    labels_gallery = labels_gallery.tolist()\n",
    "    labels_query = labels_query.tolist()\n",
    "\n",
    "    preds = convert_indices_to_labels(indices, labels_gallery)\n",
    "    score = map_per_set(labels_query, preds)\n",
    "    return [indices, score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26cfb46c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:17:07.233055Z",
     "iopub.status.busy": "2023-11-01T19:17:07.232398Z",
     "iopub.status.idle": "2023-11-01T19:20:23.879841Z",
     "shell.execute_reply": "2023-11-01T19:20:23.878785Z"
    },
    "papermill": {
     "duration": 196.659618,
     "end_time": "2023-11-01T19:20:23.882375",
     "exception": false,
     "start_time": "2023-11-01T19:17:07.222757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:43<00:00,  8.77s/it]\n",
      "100%|██████████| 8/8 [00:58<00:00,  7.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:34<00:00,  6.97s/it]\n",
      "100%|██████████| 8/8 [00:55<00:00,  6.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing indices...\n",
      "Average mAP score is : 0.6244616709732989\n"
     ]
    }
   ],
   "source": [
    "[preds, score] = predict(model1, model2)\n",
    "print(f'Average mAP score is : {score}')\n",
    "\n",
    "preds = pd.DataFrame(preds)\n",
    "preds.to_csv('preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa9b0a",
   "metadata": {
    "papermill": {
     "duration": 0.012271,
     "end_time": "2023-11-01T19:20:23.907359",
     "exception": false,
     "start_time": "2023-11-01T19:20:23.895088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 400.680356,
   "end_time": "2023-11-01T19:20:27.318859",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T19:13:46.638503",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
