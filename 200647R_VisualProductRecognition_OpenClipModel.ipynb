{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thenujan-Nagaratnam/DS_Project_Computer_Vision/blob/OpenClip/200647R_VisualProductRecognition_OpenClipModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpE-uFqbGdg6",
        "outputId": "1f689954-5976-45d2-e128-e4a0e8e893db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-gpu"
      ],
      "id": "cpE-uFqbGdg6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIOde-ZZCTyA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import faiss\n",
        "import copy\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0,\n",
        "                 m=0.50, easy_margin=False, ls_eps=0.0, device=torch.device('cuda')):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.device = device\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps  # label smoothing\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device=self.device)\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) ------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "\n",
        "        return output\n",
        "\n",
        "class DenseCrossEntropy(nn.Module):\n",
        "    def forward(self, x, target):\n",
        "        x = x.float()\n",
        "        target = target.float()\n",
        "        logprobs = torch.nn.functional.log_softmax(x, dim=-1)\n",
        "\n",
        "        loss = -logprobs * target\n",
        "        loss = loss.sum(-1)\n",
        "        return loss.mean()\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        x = x.float()\n",
        "        target = target.float()\n",
        "        probs = torch.nn.functional.softmax(x, dim=-1)\n",
        "        logprobs = torch.log(probs)\n",
        "\n",
        "        loss = -logprobs * target * (1 - probs) ** self.gamma\n",
        "        loss = loss.sum(-1)\n",
        "        return loss.mean()\n",
        "\n",
        "class ArcMarginProduct_subcenter(nn.Module):\n",
        "    def __init__(self, in_features, out_features, k=3):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
        "        self.reset_parameters()\n",
        "        self.k = k\n",
        "        self.out_features = out_features\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, features):\n",
        "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
        "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
        "        cosine, _ = torch.max(cosine_all, dim=2)\n",
        "        return cosine\n",
        "\n",
        "class ArcFaceLossAdaptiveMargin(nn.modules.Module):\n",
        "    def __init__(self, margins, s=30.0, crit='ce'):\n",
        "        super().__init__()\n",
        "        if crit == 'ce':\n",
        "            self.crit = DenseCrossEntropy()\n",
        "        else:\n",
        "            self.crit = FocalLoss()\n",
        "        self.s = s\n",
        "        self.margins = margins\n",
        "\n",
        "    def forward(self, logits, labels, out_dim):\n",
        "        ms = []\n",
        "        ms = self.margins[labels.cpu().numpy()]\n",
        "        cos_m = torch.from_numpy(np.cos(ms)).float().cuda()\n",
        "        sin_m = torch.from_numpy(np.sin(ms)).float().cuda()\n",
        "        th = torch.from_numpy(np.cos(math.pi - ms)).float().cuda()\n",
        "        mm = torch.from_numpy(np.sin(math.pi - ms) * ms).float().cuda()\n",
        "        labels = F.one_hot(labels, out_dim).float()\n",
        "        logits = logits.float()\n",
        "        cosine = logits\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * cos_m.view(-1,1) - sine * sin_m.view(-1,1)\n",
        "        phi = torch.where(cosine > th.view(-1,1), phi, cosine - mm.view(-1,1))\n",
        "        output = (labels * phi) + ((1.0 - labels) * cosine)\n",
        "        output *= self.s\n",
        "        loss = self.crit(output, labels)\n",
        "        return loss\n",
        "\n",
        "def set_seed(seed):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "\n",
        "def get_similiarity_hnsw(embeddings_gallery, emmbeddings_query, k):\n",
        "    # this is guy is really fast\n",
        "    print('Processing indices...')\n",
        "\n",
        "    s = time.time()\n",
        "    index = faiss.IndexHNSWFlat(embeddings_gallery.shape[1], 32)\n",
        "    index.add(embeddings_gallery)\n",
        "\n",
        "    scores, indices = index.search(emmbeddings_query, k)\n",
        "    e = time.time()\n",
        "\n",
        "    print(f'Finished processing indices, took {e - s}s')\n",
        "    return scores, indices\n",
        "\n",
        "def get_similiarity_l2(embeddings_gallery, emmbeddings_query, k):\n",
        "    print('Processing indices...')\n",
        "\n",
        "    s = time.time()\n",
        "    index = faiss.IndexFlatL2(embeddings_gallery.shape[1])\n",
        "    index.add(embeddings_gallery)\n",
        "\n",
        "    scores, indices = index.search(emmbeddings_query, k)\n",
        "    e = time.time()\n",
        "\n",
        "    print(f'Finished processing indices, took {e - s}s')\n",
        "    return scores, indices\n",
        "\n",
        "\n",
        "def get_similiarity_IP(embeddings_gallery, emmbeddings_query, k):\n",
        "    print('Processing indices...')\n",
        "\n",
        "    s = time.time()\n",
        "    index = faiss.IndexFlatIP(embeddings_gallery.shape[1])\n",
        "    index.add(embeddings_gallery)\n",
        "\n",
        "    scores, indices = index.search(emmbeddings_query, k)\n",
        "    e = time.time()\n",
        "\n",
        "    print(f'Finished processing indices, took {e - s}s')\n",
        "    return scores, indices\n",
        "\n",
        "def get_similiarity(embeddings, k):\n",
        "    print('Processing indices...')\n",
        "\n",
        "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "\n",
        "    res = faiss.StandardGpuResources()\n",
        "\n",
        "    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
        "\n",
        "    index.add(embeddings)\n",
        "\n",
        "    scores, indices = index.search(embeddings, k)\n",
        "    print('Finished processing indices')\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def map_per_image(label, predictions, k=5):\n",
        "    try:\n",
        "        return 1 / (predictions[:k].index(label) + 1)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "def map_per_set(labels, predictions, k=5):\n",
        "    return np.mean([map_per_image(l, p, k) for l,p in zip(labels, predictions)])\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, window_size=None):\n",
        "        self.length = 0\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.window_size = window_size\n",
        "\n",
        "    def reset(self):\n",
        "        self.length = 0\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        if self.window_size and (self.count >= self.window_size):\n",
        "            self.reset()\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def get_lr_groups(param_groups):\n",
        "        groups = sorted(set([param_g['lr'] for param_g in param_groups]))\n",
        "        groups = [\"{:2e}\".format(group) for group in groups]\n",
        "        return groups\n",
        "\n",
        "def convert_indices_to_labels(indices, labels):\n",
        "    indices_copy = copy.deepcopy(indices)\n",
        "    for row in indices_copy:\n",
        "        for j in range(len(row)):\n",
        "            row[j] = labels[row[j]]\n",
        "    return indices_copy\n",
        "\n",
        "class Multisample_Dropout(nn.Module):\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super(Multisample_Dropout, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.dropouts = nn.ModuleList([nn.Dropout((i+1)*.1) for i in range(5)])\n",
        "\n",
        "    def forward(self, x, module):\n",
        "        x = self.dropout(x)\n",
        "        return torch.mean(torch.stack([module(dropout(x)) for dropout in self.dropouts],dim=0),dim=0)\n",
        "\n",
        "def transforms_auto_augment(image_path, image_size):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    train_transforms = transforms.Compose([transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET), transforms.PILToTensor()])\n",
        "    return train_transforms(image)\n",
        "\n",
        "def transforms_cutout(image_path, image_size):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "    train_transforms = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
        "            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.7),\n",
        "            A.Resize(image_size, image_size),\n",
        "            A.Cutout(max_h_size=int(image_size * 0.4), max_w_size=int(image_size * 0.4), num_holes=1, p=0.5),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    return train_transforms(image=image)['image']\n",
        "\n",
        "def transforms_happy_whale(image_path, image_size):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
        "    aug8p3 = A.OneOf([\n",
        "            A.Sharpen(p=0.3),\n",
        "            A.ToGray(p=0.3),\n",
        "            A.CLAHE(p=0.3),\n",
        "        ], p=0.5)\n",
        "\n",
        "    train_transforms = A.Compose([\n",
        "            A.ShiftScaleRotate(rotate_limit=15, scale_limit=0.1, border_mode=cv2.BORDER_REFLECT, p=0.5),\n",
        "            A.Resize(image_size, image_size),\n",
        "            aug8p3,\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    return train_transforms(image=image)['image']\n",
        "\n",
        "def transforms_valid(image_path, image_size):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    valid_transforms = transforms.Compose([transforms.PILToTensor()])\n",
        "    return valid_transforms(image)"
      ],
      "id": "UIOde-ZZCTyA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d87b266d"
      },
      "source": [
        "# MODELS TO BE ENSEMBLE"
      ],
      "id": "d87b266d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3643903b"
      },
      "outputs": [],
      "source": [
        "# path1 is better model\n",
        "path1 = '/content/drive/MyDrive/DS_project/model1.pt'"
      ],
      "id": "3643903b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAtkEqRHCf84",
        "outputId": "953b681a-2c7e-498d-9107-22eccee4bf8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "GAtkEqRHCf84"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgzz0s2ZC4jD",
        "outputId": "69b649ce-38aa-43f0-c6a2-5f276466d425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open-clip-torch\n",
            "  Downloading open_clip_torch-2.20.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (0.15.2+cu118)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (2023.6.3)\n",
            "Collecting ftfy (from open-clip-torch)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (4.66.1)\n",
            "Collecting huggingface-hub (from open-clip-torch)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from open-clip-torch)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4 in /usr/local/lib/python3.10/dist-packages (from open-clip-torch) (3.20.3)\n",
            "Collecting timm (from open-clip-torch)\n",
            "  Downloading timm-0.9.5-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->open-clip-torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open-clip-torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->open-clip-torch) (16.0.6)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->open-clip-torch) (0.2.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open-clip-torch) (23.1)\n",
            "Collecting safetensors (from timm->open-clip-torch)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (1.23.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->open-clip-torch) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->open-clip-torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->open-clip-torch) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
            "Installing collected packages: sentencepiece, safetensors, ftfy, huggingface-hub, timm, open-clip-torch\n",
            "Successfully installed ftfy-6.1.1 huggingface-hub-0.16.4 open-clip-torch-2.20.0 safetensors-0.3.2 sentencepiece-0.1.99 timm-0.9.5\n"
          ]
        }
      ],
      "source": [
        "pip install open-clip-torch"
      ],
      "id": "qgzz0s2ZC4jD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UwiBuWgC4l7"
      },
      "outputs": [],
      "source": [],
      "id": "6UwiBuWgC4l7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RveiLL-0C4rd"
      },
      "outputs": [],
      "source": [],
      "id": "RveiLL-0C4rd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3wMtlMEC4ue"
      },
      "outputs": [],
      "source": [],
      "id": "K3wMtlMEC4ue"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOeoRsLNC41W"
      },
      "outputs": [],
      "source": [],
      "id": "KOeoRsLNC41W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cc3868da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import open_clip\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n"
      ],
      "id": "cc3868da"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cccba01"
      },
      "outputs": [],
      "source": [
        "# get product 10k\n",
        "\n",
        "def read_img(img_path, is_gray=False):\n",
        "    mode = cv2.IMREAD_COLOR if not is_gray else cv2.IMREAD_GRAYSCALE\n",
        "    img = cv2.imread(img_path, mode)\n",
        "    if not is_gray:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return img\n",
        "\n",
        "class ProductDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 img_dir,\n",
        "                 annotations_file,\n",
        "                 transform=None,\n",
        "                 final_transform=None,\n",
        "                 headers=None,\n",
        "                 test_mode=False):\n",
        "        self.data = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.final_transform = final_transform\n",
        "        self.headers = {\"img_path\": \"img_path\", \"product_id\": \"product_id\"}\n",
        "        if headers:\n",
        "            self.headers = headers\n",
        "        self.test_mode = test_mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.data[self.headers[\"img_path\"]][idx])\n",
        "\n",
        "        img = read_img(img_path)\n",
        "        if self.test_mode:\n",
        "            x, y, w, h = self.data[\"bbox_x\"][idx], self.data[\"bbox_y\"][idx], \\\n",
        "                         self.data[\"bbox_w\"][idx], self.data[\"bbox_h\"][idx]\n",
        "            img = img[y:y+h, x:x+w]\n",
        "\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = transform(image=img)[\"image\"]\n",
        "\n",
        "        if self.final_transform is not None:\n",
        "            if isinstance(img, np.ndarray):\n",
        "                img =  Image.fromarray(img)\n",
        "            img = self.final_transform(img)\n",
        "\n",
        "        product_id = self.data[self.headers[\"product_id\"]][idx]\n",
        "        return img, product_id\n",
        "\n",
        "def get_final_transform():\n",
        "    final_transform = T.Compose([\n",
        "            T.Resize(\n",
        "                size=(224, 224),\n",
        "                interpolation=T.InterpolationMode.BICUBIC,\n",
        "                antialias=True),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(\n",
        "                mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                std=(0.26862954, 0.26130258, 0.27577711)\n",
        "            )\n",
        "        ])\n",
        "    return final_transform\n",
        "\n",
        "@th.no_grad()\n",
        "def extract_embeddings(model, dataloader, epoch=10, use_cuda=True):\n",
        "    features = []\n",
        "    product_id = []\n",
        "\n",
        "    for _ in range(epoch):\n",
        "        for imgs, p_id in tqdm(dataloader):\n",
        "            if use_cuda:\n",
        "                imgs = imgs.cuda()\n",
        "            features.append(th.squeeze(model(imgs.half())).detach().cpu().numpy().astype(np.float32))\n",
        "            product_id.append(th.squeeze(p_id).detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    return np.concatenate(features, axis=0), np.concatenate(product_id)"
      ],
      "id": "0cccba01"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9623a3"
      },
      "source": [
        "# ENSEMBLE STEP"
      ],
      "id": "fc9623a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keLNhUw3DXzA"
      },
      "outputs": [],
      "source": [],
      "id": "keLNhUw3DXzA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqeMNHDeHwoF",
        "outputId": "e5e9eaed-3675-4dfa-91ac-16e4f84fef53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1OpTvvc66Olx4P2oktK40emmJ99E0h0jF\n",
            "To: /content/test-archive.zip\n",
            "100% 441M/441M [00:03<00:00, 131MB/s] \n"
          ]
        }
      ],
      "source": [
        "! gdown 1OpTvvc66Olx4P2oktK40emmJ99E0h0jF"
      ],
      "id": "gqeMNHDeHwoF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AfZUm84H8fi"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip /content/test-archive.zip -d /content/testing-dataset"
      ],
      "id": "7AfZUm84H8fi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1YdxPBlIF2j"
      },
      "outputs": [],
      "source": [
        "!rm /content/test-archive.zip"
      ],
      "id": "X1YdxPBlIF2j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szkEX8qci9LL"
      },
      "outputs": [],
      "source": [
        "!mv \"/content/testing-dataset/development_test_data/gallery\" \"/content/\"\n",
        "!mv \"/content/testing-dataset/development_test_data/queries/\" \"/content/\"\n",
        "!mv \"/content/testing-dataset/development_test_data/gallery.csv\" \"/content/\"\n",
        "!mv \"/content/testing-dataset/development_test_data/queries.csv\" \"/content/\"\n",
        "\n",
        "!rmdir \"/content/testing-dataset/development_test_data\"\n",
        "\n",
        "!mv \"/content/gallery\" \"/content/testing-dataset/\"\n",
        "!mv \"/content/queries\" \"/content/testing-dataset/\"\n",
        "!mv \"/content/gallery.csv\" \"/content/testing-dataset/\"\n",
        "!mv \"/content/queries.csv\" \"/content/testing-dataset/\""
      ],
      "id": "szkEX8qci9LL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCJEbjcLDX1-"
      },
      "outputs": [],
      "source": [],
      "id": "DCJEbjcLDX1-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSMQW3kXDX5E"
      },
      "outputs": [],
      "source": [],
      "id": "wSMQW3kXDX5E"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CZWI0LNDX8B"
      },
      "outputs": [],
      "source": [],
      "id": "2CZWI0LNDX8B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6758306",
        "outputId": "4ce5d794-ad6e-4a47-9bda-74f47681e365"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patchnorm_pre_ln): Identity()\n",
              "  (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "  (patch_dropout): Identity()\n",
              "  (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): ModuleList(\n",
              "      (0-31): 32 x ResidualAttentionBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (ls_1): Identity()\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "        (ls_2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "backbone = open_clip.create_model_and_transforms('ViT-H-14', None)[0].visual\n",
        "backbone.load_state_dict(th.load(path1))\n",
        "backbone.half()\n",
        "backbone.eval()"
      ],
      "id": "d6758306"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f74f030",
        "outputId": "bce32b82-397e-40cb-c290-9d8dcf961287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "final_transform = get_final_transform()\n",
        "img_dir = \"/content/testing-dataset\"\n",
        "dataset_test = ProductDataset(img_dir, os.path.join(img_dir, \"queries.csv\"), None, final_transform, test_mode=True)\n",
        "dataloader_test = DataLoader(dataset_test, batch_size=512, num_workers=4)\n",
        "\n",
        "dataset_train = ProductDataset(img_dir, os.path.join(img_dir, \"gallery.csv\"), None, final_transform)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=512, num_workers=4)\n",
        "\n",
        "# img_dir = \"../amazon_dataset_1\"\n",
        "# headers = {\"img_path\": \"path\", \"product_id\": \"id\"}\n",
        "# dataset_test = ProductDataset(img_dir, os.path.join(img_dir, \"query.csv\"), None, final_transform, headers=headers)\n",
        "# dataloader_amazon_test = DataLoader(dataset_test, batch_size=512, num_workers=4)\n",
        "\n",
        "# dataset_train = ProductDataset(img_dir, os.path.join(img_dir, \"gallery.csv\"), None, final_transform, headers=headers)\n",
        "# dataloader_amazon_train = DataLoader(dataset_train, batch_size=512, num_workers=4)"
      ],
      "id": "5f74f030"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0301d7bf"
      },
      "outputs": [],
      "source": [
        "# x = []\n",
        "@th.no_grad()\n",
        "def compute_score_test_data(model):\n",
        "    embeddings_query, labels_query = extract_embeddings(model, dataloader_test, 1)\n",
        "    embeddings_gallery, labels_gallery = extract_embeddings(model, dataloader_train, 1)\n",
        "\n",
        "    _, indices = get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n",
        "\n",
        "\n",
        "    indices = indices.tolist()\n",
        "    labels_gallery = labels_gallery.tolist()\n",
        "    labels_query = labels_query.tolist()\n",
        "\n",
        "    preds = convert_indices_to_labels(indices, labels_gallery)\n",
        "    score = map_per_set(labels_query, preds)\n",
        "    return score\n",
        "\n",
        "# @th.no_grad()\n",
        "# def compute_score_amazon_data(model):\n",
        "#     embeddings_query, labels_query = extract_embeddings(model, dataloader_amazon_test, 1)\n",
        "#     embeddings_gallery, labels_gallery = extract_embeddings(model, dataloader_amazon_train, 1)\n",
        "\n",
        "#     _, indices = get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n",
        "\n",
        "\n",
        "#     indices = indices.tolist()\n",
        "#     labels_gallery = labels_gallery.tolist()\n",
        "#     labels_query = labels_query.tolist()\n",
        "\n",
        "#     preds = convert_indices_to_labels(indices, labels_gallery)\n",
        "#     score = map_per_set(labels_query, preds)\n",
        "\n",
        "#     return score"
      ],
      "id": "0301d7bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21192841"
      },
      "outputs": [],
      "source": [
        "# n = 20\n",
        "# W = [i/n for i in range(0, n+1)]\n",
        "# m_ap_test = []\n",
        "# m_ap_amazon = []\n",
        "\n",
        "# with th.no_grad():\n",
        "#     for w in W:\n",
        "#         model = open_clip.create_model_and_transforms('ViT-H-14', None)[0].visual\n",
        "#         model.load_state_dict(th.load(path2))\n",
        "#         model.half()\n",
        "#         model.eval()\n",
        "\n",
        "#         for o, b in zip(model.parameters(), backbone.parameters()):\n",
        "#             o.data = w*o + (1 - w)*b\n",
        "\n",
        "#         model.cuda()\n",
        "#         #score_amazon = compute_score_amazon_data(model)\n",
        "#         score_amazon = 0\n",
        "#         score_test = compute_score_test_data(model)\n",
        "\n",
        "#         print(f'Weight {w} - test score {score_test} | amazon score {score_amazon}')\n",
        "#         m_ap_test.append(score_test)\n",
        "#         #m_ap_amazon.append(score_amazon)\n"
      ],
      "id": "21192841"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc6cc435",
        "outputId": "c1f8f797-e55a-45a8-ce1b-f467cd114d8d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patchnorm_pre_ln): Identity()\n",
              "  (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "  (patch_dropout): Identity()\n",
              "  (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): ModuleList(\n",
              "      (0-31): 32 x ResidualAttentionBlock(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (ls_1): Identity()\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "        )\n",
              "        (ls_2): Identity()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model = backbone\n",
        "model.cuda()\n",
        "# score_amazon = compute_score_amazon_data(model)\n",
        "# score_test = compute_score_test_data(model)\n",
        "# score_test"
      ],
      "id": "cc6cc435"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnAZjpMXNgY0"
      },
      "outputs": [],
      "source": [
        "def predict(model):\n",
        "    embeddings_query, labels_query = extract_embeddings(model, dataloader_test, 1)\n",
        "    embeddings_gallery, labels_gallery = extract_embeddings(model, dataloader_train, 1)\n",
        "\n",
        "    _, indices = get_similiarity_l2(embeddings_gallery, embeddings_query, 1000)\n",
        "\n",
        "\n",
        "    indices = indices.tolist()\n",
        "    labels_gallery = labels_gallery.tolist()\n",
        "    labels_query = labels_query.tolist()\n",
        "\n",
        "    preds = convert_indices_to_labels(indices, labels_gallery)\n",
        "    score = map_per_set(labels_query, preds)\n",
        "    return [indices, score]"
      ],
      "id": "BnAZjpMXNgY0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMQlTX3LNzTN",
        "outputId": "9d07e7e6-5d45-4f8a-e24b-29d522696898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [01:26<00:00, 21.67s/it]\n",
            "100%|██████████| 3/3 [00:40<00:00, 13.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing indices...\n",
            "Finished processing indices, took 0.3877840042114258s\n"
          ]
        }
      ],
      "source": [
        "[preds, score] = predict(model)\n",
        "# Create a DataFrame from the predictions\n",
        "df = pd.DataFrame(preds)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('preds.csv', index=False)"
      ],
      "id": "LMQlTX3LNzTN"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}